[{"content":"Hi 👋 # 🏢 Ingénieur DevOps en alternance @Claranet France\nPassionné des technologies cloud et de tout ce qui touche à l’open-source en général, le partage me tient à coeur et j\u0026rsquo;aime prendre du temps pour aider les autres.\n","date":null,"permalink":"/","section":"/home","summary":"Hi 👋 # 🏢 Ingénieur DevOps en alternance @Claranet France","title":"/home"},{"content":" Réalisé dans le cadre de mon année de Bachelor, le Hackaton à pour but de proposer une solution informatique en une semaine. Un jury juge ensuite la solution la plus convaincante et désigne les vainqueurs.\nL\u0026rsquo;ensemble du projet que j\u0026rsquo;ai réalisé est disponible en open-source\nsur GitLab\nIntroduction # Lors de ce hackathon, mon équipe à choisi pour sujet le développement d’un frontend via l’intégration de différentes technologies et la mise en place d’une plateforme SaaS avec toutes les contraintes que ça comporte (passage à l’échelle d’un PoC vers une plateforme réelle). Le projet a pour objectif de résoudre plusieurs problèmes liés aux supports de formation, à la fois côté conception mais aussi pour leur conservation et adaptation côté école.\nContexte et travail d\u0026rsquo;équipe # L\u0026rsquo;ensemble des étudiants de l\u0026rsquo;école sont réunis en équipe de 10, tout niveaux et spécialités confondues. Toute la difficulté étant de coordonner l\u0026rsquo;ensemble de l\u0026rsquo;équipe afin de livrer un produit fonctionnel. Ainsi, il était demandé de :\nRéaliser un front-end applicatif en utilisant un backend fourni par l\u0026rsquo;école Réaliser l\u0026rsquo;infrastructure hébergeant le back et le front Fournir une fonctionnalité de livraison et d\u0026rsquo;intégration continue (CI/CD) Limites et difficultés # Malheureusement, l\u0026rsquo;équipe dont j\u0026rsquo;ai fais partie n\u0026rsquo;as pas sû délivrer un front-end dans les temps impartis. Cela dit, de mon côté j\u0026rsquo;ai pu architecturer l\u0026rsquo;ensemble de l\u0026rsquo;infrastructure ainsi que la pipeline CI/CD et obtenir un POC fonctionnel. Il ne me manquais plus qu\u0026rsquo;un front-end afin d\u0026rsquo;obtenir le résultat demandé, dommage 😥!\nCependant, j\u0026rsquo;ai pu tester le fonctionnement du back-end sur l\u0026rsquo;architecture ainsi déployée. C\u0026rsquo;est d\u0026rsquo;ailleurs l\u0026rsquo;objet de cet article : présenter ma solution sur la partie DevOps 😎.\nSchéma d\u0026rsquo;architecture # J\u0026rsquo;ai réalisé un petit brouillon de l\u0026rsquo;architecture que j\u0026rsquo;ai souhaité mettre en place sur le cloud Azure :\nOn à donc:\nUn cluster Kubernetes managé sur le cloud Azure (AKS) 3 namepsaces : un pour le front, un pour le back et le dernier pour le monitoring et la gestion des logs Lors de la réalisation, j\u0026rsquo;ai regroupé tout mes pods K8S au sein du même namespace par manque de temps. Ce projet n\u0026rsquo;est de plus pas du tout destiné à être mis en production. Il manque d\u0026rsquo;ailleurs aussi toute la partie monitoring avec Elasticsearch/Prometheus. En l\u0026rsquo;état, l\u0026rsquo;architecture disponible sur le lien GitLab donné en préambule permet uniquement de requêter l\u0026rsquo;API du backend. De plus, comme indiqué sur le schéma la base de donnée utilisée est hébergée dans un pod. J\u0026rsquo;aurais préféré mettre en place une base de donnée managée mais n\u0026rsquo;étant pas encore très à l\u0026rsquo;aise avec Kubernetes sur Azure, j\u0026rsquo;ai préféré aller au plus simple.\nIAC : Terraform # L\u0026rsquo;enjeu étant de provisioner tout cela as code, j\u0026rsquo;ai déployé cette architecture avec Terraform. Le repo contenant l\u0026rsquo;infrastructure est construit comme ceci:\n├── README.md └── terraform ├── environment │ └── dev │ └── variables.tfvars ├── main.tf ├── modules │ └── kubernetes │ ├── main.tf │ ├── outputs.tf │ └── variables.tf ├── outputs.tf └── variables.tf Cette architecture est dès le départ conçue pour être en mesure de déployer la même infrastructure en fonction de l\u0026rsquo;environement de production choisi. Pour la maquette, j\u0026rsquo;ai seulement réalisé la dev.\nCI/CD # J\u0026rsquo;ai développé une CI permettant depuis le repo hébergeant le back-end de :\nTester le code Builder l\u0026rsquo;image Docker contenant le code hébergeant l\u0026rsquo;API Push cet image sur le repository du projet GitLab Trigger la CI du repo d\u0026rsquo;infrastructure en fournissant l\u0026rsquo;ID de l\u0026rsquo;image précédemment créée. Pour cela, il m\u0026rsquo;a fallu containeriser l\u0026rsquo;application en passant par l\u0026rsquo;édition d\u0026rsquo;un Dockerfile.\nLe code est consultable ici\nDe même, côté infra la CI permet tout simplement de plan et apply la configuration Terraform. A terme, les images du front et backend seront fournies lors du trigger de cette CI depuis les différents repo des développeurs.\nLe code est consultable ici\nL\u0026rsquo;idée derrière ce POC étant de permettre à un développeur de push ses modifications du code back ou front, et d\u0026rsquo;ainsi automatiquement déployer l\u0026rsquo;applicatif sur le cluster Kubernetes.\nFinalement # Cette semaine fût assez intense, mais m\u0026rsquo;as permis de réaliser un début de lab sur des technologies que je n\u0026rsquo;ai pas encore eu l\u0026rsquo;occasion de tâter lors de mes différentes missions en entreprise. J\u0026rsquo;ai d\u0026rsquo;ailleurs prévu de continuer ce POC afin d\u0026rsquo;en faire un article plus détaillé sur AKS et la solution de CI/CD proposée par GitLab.\n","date":"3 juillet 2023","permalink":"/projets/hackaton_sdv/","section":"Projets","summary":"Réalisé dans le cadre de mon année de Bachelor, le Hackaton à pour but de proposer une solution informatique en une semaine.","title":"Hackaton Sup De Vinci 2023"},{"content":" Cette page rassemble l\u0026rsquo;ensemble de mes projets scolaires et personnels réalisés en classe ou sur mon temps libre ","date":null,"permalink":"/projets/","section":"Projets","summary":" Cette page rassemble l\u0026rsquo;ensemble de mes projets scolaires et personnels réalisés en classe ou sur mon temps libre ","title":"Projets"},{"content":"","date":null,"permalink":"/categories/aws/","section":"Categories","summary":"","title":"aws"},{"content":"","date":null,"permalink":"/tags/awscli/","section":"Tags","summary":"","title":"awscli"},{"content":" C\u0026rsquo;est ici que je publie mes articles quand le temps me le permet. 📌 Articles récents # ","date":null,"permalink":"/blog/","section":"Blog","summary":"C\u0026rsquo;est ici que je publie mes articles quand le temps me le permet.","title":"Blog"},{"content":"","date":null,"permalink":"/categories/","section":"Categories","summary":"","title":"Categories"},{"content":"","date":null,"permalink":"/tags/cloud/","section":"Tags","summary":"","title":"cloud"},{"content":"","date":null,"permalink":"/tags/cloudformation/","section":"Tags","summary":"","title":"cloudformation"},{"content":"","date":null,"permalink":"/tags/ec2/","section":"Tags","summary":"","title":"ec2"},{"content":"","date":null,"permalink":"/tags/gaming/","section":"Tags","summary":"","title":"gaming"},{"content":"Introduction # Le cloud-gaming se démocratise de plus en plus notamment par le biais d\u0026rsquo;acteurs tel que OVH avec son offre Shadow, ou encore NVDIA avec GeForce Now. Je me suis toujours demandé par quel procédé il serais possible de créer mon propre serveur cloud dédié au jeu, et surtout si c\u0026rsquo;était viable. Après quelques recherches, j\u0026rsquo;ai découvert quelques pistes assez intéressantes, notament du côté d\u0026rsquo;AWS qui propose des instances de calcul avec GPU plutôt abordables : les instances G4.\nAttention à la facture si tu oublies d\u0026rsquo;éteindre ou de supprimer ton instance après avoir déroulé l\u0026rsquo;article! Cet article te permettra de monter ta propre VM GPU, tout en te faisant découvrir aws cli et CloudFormation, l\u0026rsquo;outil d\u0026rsquo;IAC proposé par AWS.\nLes types d\u0026rsquo;instances G4 # AWS propose deux architectures, l\u0026rsquo;une est basée sur les puces GPU NVIDIA T4 (g4dn), l\u0026rsquo;autre sur des puces AMD Radeon Pro V520 (g4ad). Ci-dessous un tableau comparatif du rapport prix/performances des différentes instances :\nInstance Type 3DMark Score On-demand Price (us-east-1, USD, 02/23) Price-performance (3DMark points / $) g4dn.xlarge 4300 $0.71 6056 g4dn.2xlarge 4800 $1.12 4286 g4dn.4xlarge 6000 $1.94 3093 g4ad.xlarge 5100 $0.56 9107 g4ad.2xlarge 6600 $0.91 7253 g4ad.4xlarge 7600 $1.60 4750 g5.xlarge 6800 $1.19 5714 g5.2xlarge 10200 $1.58 6456 g5.4xlarge 13000 $2.36 5508 Source : https://github.com/aws-samples/cloud-gaming-on-ec2-instances\nPour cet article, je me base sur une instance AMD g4ad.xlarge, largement suffisante pour mes besoins.\nPrérequis # Un compte AWS awscli d\u0026rsquo;installé et de configuré (voir la doc AWS) Assez de quota sur ton compte AWS pour provisionner des instances GPU G4 (j\u0026rsquo;explique plus bas comment en obtenir) A noter: Il n\u0026rsquo;y a pas de free tier proposé par AWS sur les instances GPU. Il est essentiel de bien éteindre ou détruire ta VM une fois la session terminée afin éviter d\u0026rsquo;être facturé pour des heures inutilisées\u0026hellip; Infrastructure cible # Mon template CloudFormation permet de générer :\nUn VPC dédié Un subnet public Une instance EC2 g4adn.xlarge avec le système sur un disque SSD, avec un volume HDD (st1) monté pour stocker les jeux Je me suis basé sur l\u0026rsquo;ami Microsoft Windows Server 2019 with AMD Radeon Pro Driver proposée gratuitement par AWS pour générer mon template. Elle est préconfigurée avec les drivers AMD préinstallés (pratique).\nLes disques st1 sont relativement lents, mais peu cher. J\u0026rsquo;ai choisi cette option ici afin de tester ma config, mais il est conseillé d\u0026rsquo;utiliser un stockage SSD pour jouer confortablement. Déploiement # Quotas de services # Les instances G4 AWS nécessitent des quotas pour être provisionnées. Par défaut, sur ce type d\u0026rsquo;instance, le quota est de 0. Si tu essayes de provisionner une machine G4 en ayant 0 quota, ça ne marchera pas. Pour ce faire, connecte toi à la console AWS et tape service quotas dans la barre de recherche et choisi EC2 :\nDans la barre de recherche sur l\u0026rsquo;écran suivant, tape \u0026ldquo;G4\u0026rdquo; et suit les instructions pour augmenter le quota. Le quota correspond au nombres de vCPU allouées à une instance. Pour la g4ad.xlarge, il nous faut 4 vCPU. Changer la valeur du quota par 4, et envoyer.\nIl n\u0026rsquo;est pas possible d\u0026rsquo;obtenir des quotas pour des host dedicated avec un compte AWS personnel qui a peu servi. Après avoir fait la demande, le service client d\u0026rsquo;AWS la refusera, et te proposera des quotas pour des instances de type spot ou on-demand. J\u0026rsquo;ai demandé d\u0026rsquo;avoir accès aux instances on-demand suite à ma première requête, celles-ci furent disponible le lendemain. Déploiement de l\u0026rsquo;EC2 # Si ce n\u0026rsquo;est pas déjà fais, authentifie toi à awscli. Si tu as besoin d\u0026rsquo;aide, la doc d\u0026rsquo;Amazon est vraiment bien faite.\nÉtant sous MacOS, les commandes suivantes devraient fonctionner sous Linux et MacOS. Si tu es sous Windows, je te conseille de passer via WSL pour bénéficier d\u0026rsquo;un shell bash. Mon template utilise une KeyPair appelée CloudGamingKeyPair permettant de decrypter le mot de passe administrateur de l\u0026rsquo;instance. Pour la créer :\nmkdir ~/.ssh/aws-private-keys \u0026amp;\u0026amp; cd ~/.ssh/aws-private-keys aws ec2 create-key-pair \\ --key-name CloudGamingKeyPair \\ --query \u0026#39;KeyMaterial\u0026#39; \\ --region eu-west-2 \\ --output text \u0026gt; CloudGamingKeyPair.pem Clone mon repo et ce rendre dans le dossier contenant le template :\ngit clone https://github.com/fabienchevalier/ec2-cloudgaming \u0026amp;\u0026amp; cd cloudformation Ouvre le template avec un editeur de texte, et modifie les lignes 60 et 64 en remplacant l\u0026rsquo;adresse par ton adresse IP.\nSecurityGroupIngress: - IpProtocol: tcp FromPort: 8443 #NICE DVC Server ToPort: 8443 CidrIp: 0.0.0.0/0 #Replace that with your IP address (mask should be /32) - IpProtocol: tcp FromPort: 3389 ToPort: 3389 CidrIp: 0.0.0.0/0 #Repl 0.0.0.0/0 autorise n\u0026rsquo;importe quelle adresse sur le port RDP et NICE DCV Server, ce qui n\u0026rsquo;est pas souhaitable.\nTip: tu peux récupérer ton adresse ip public via curl ifconfig.me Déploie le template CloudFormation :\naws --region eu-west-2 cloudformation deploy \\ --template deploy-cloud-gaming-ec2.cfn.yaml \\ --stack-name CloudGamingStack Il est possible de suivre l\u0026rsquo;avancement de la création de la stack sur la console AWS, via la page CloudFormation : Une fois l\u0026rsquo;instance déployée, il faut récupérer le mot de passe Administrateur permettant une première connexion via le protocole RDP.\nOn récupère l\u0026rsquo;ID de l\u0026rsquo;instance crée :\naws --region eu-west-2 ec2 describe-instances \\ --filters \u0026#34;Name=tag:Name,Values=CloudGamingInstance\u0026#34; \\ --query \u0026#39;Reservations[].Instances[].[InstanceId]\u0026#39; \\ --output text Copier l\u0026rsquo;ID généré puis :\naws ec2 get-password-data --instance-id i-1234567890abcdef0 \\ --priv-launch-key ~/.ssh/aws-private-keys/CloudGamingKeyPair.pem Le mot de passe Administrateur se situe dans le champ PasswordData de l\u0026rsquo;output, par exemple :\n{ \u0026#34;InstanceId\u0026#34;: \u0026#34;i-1234567890abcdef0\u0026#34;, \u0026#34;Timestamp\u0026#34;: \u0026#34;2013-08-30T23:18:05.000Z\u0026#34;,z \u0026#34;PasswordData\u0026#34;: \u0026#34;\u0026amp;ViJ652e*u\u0026#34; } On y est presque!\nConfiguration de l\u0026rsquo;instance # Disque dur # Une première connexion via RDP est nécessaire afin de configurer quelques derniers détails. Pour ce faire, lance un client RDP (Microsoft Remote Desktop sous MacOS pour ma part) et connecte toi à l\u0026rsquo;instance via son IP publique.\nUtilise les informations de login récupérées auparavant (login Administrator, mot de passe donné par la commande ec2 get-password-data) pour te connecter. Cherches disk manager dans la barre de recherche Windows, et formate le disque créé par le template CloudFormation comme ceci :\nChoisir l\u0026rsquo;option New Simple Volume, et formater l\u0026rsquo;ensemble de l\u0026rsquo;espace disque disponible.\nServeur Nice DCV # Il nous reste à déployer le serveur Nice DCV permettant de streamer sans latences depuis l\u0026rsquo;instance EC2. Pour ce faire, il faudra se connecter une première fois en RDP via l\u0026rsquo;adresse IP publique de l\u0026rsquo;EC2 afin d\u0026rsquo;executer ce script PowerShell :\n# Set TLS 1.2 for Invoke-RestMethod [Net.ServicePointManager]::SecurityProtocol = [Net.SecurityProtocolType]::Tls12 # Set the download link to Nice DCV Server (64-bit installer) $downloadUrl = \u0026#34;https://d1uj6qtbmh3dt5.cloudfront.net/nice-dcv-server-x64-Release.msi\u0026#34; # Set the path for our download, which will be in the temp directory $installerFile = \u0026#34;nice-dcv-server-x64-Release.msi\u0026#34; $installerDownloadPath = (Join-Path $env:TEMP $installerFile) # Set the default owner to the current user $installerOwner = [Environment]::UserName # Set Install Command Expression $msiExpression = \u0026#34;msiexec.exe /i $installerDownloadPath AUTOMATIC_SESSION_OWNER=$installerOwner ADDLOCAL=ALL /quiet /norestart /l*v dcv_install_msi.log\u0026#34; # Download the file Invoke-Webrequest $downloadUrl -UseBasicParsing -OutFile $installerDownloadPath # Install Invoke-Expression $msiExpression Pour ce faire, copie/colle ce script dans le notepad de l\u0026rsquo;instance, et enregistre le fichier sur le bureau :\nPuis, clic droit sur l\u0026rsquo;icône, puis Run with PowerShell. Attendre une ou deux minutes après que la fenêtre se ferme, le temps que le serveur se lance.\nThat\u0026rsquo;s it! Une fois le serveur déployé, tu peux fermer ta connexion RDP, et télécharger le client Nice DCV.\nConnexion à l\u0026rsquo;instance via Nice DCV # Lance le client, et connecte toi via l\u0026rsquo;adresse IP publique de ton instance, sur le port 8443:\nUne erreur de certificat peut apparaître, clique sur Trust \u0026amp; Connect\nA partir de là, libre à toi d\u0026rsquo;installer tes jeux et de commencer à jouer !\nJe te conseilles de désactiver le mode de sécurité d\u0026rsquo;IE, navigateur par défaut sur Windows Server (hé oui\u0026hellip;) puis d\u0026rsquo;installer un navigateur récent. Quelques améliorations # Par défaut, le serveur NICE DCV streame 25 FPS. C\u0026rsquo;est configurable via le registre Windows, via la clée située ici : HKEY_CURRENT_USER\\Software\\GSettings\\com\\nicesoftware\\dcv. Créer une nouvelle clée display et lui donner pour valeur (type DWORD 32bits) le nombre de FPS souhaité.\nConclusion # Après quelques heures d\u0026rsquo;essais, le confort de jeu est plutôt bon. Pas trop de latences, même en WiFi. La qualité d\u0026rsquo;image est aussi plutôt bonne.\nCependant, le coût engendré par l\u0026rsquo;utilisation de l\u0026rsquo;EC2 reste trop élevé à mon sens pour que cela soit viable. En effet, il faut ajouter au prix horaire de l\u0026rsquo;instance :\nLe stockage (assez cher si on choisit de passer par du SSD : +/- 11$/mois) La bande passante : payée au Go, ça peut vite s\u0026rsquo;envoler pour du stream 4K@60FPS. Cela dit, cette méthode de tarification à l\u0026rsquo;heure peut convenir à des joueurs occasionels ne voulant pas s\u0026rsquo;abonner à un service qu\u0026rsquo;ils utiliserons que très peu.\nIl est aussi possible d\u0026rsquo;utiliser Parsec pour streamer le contenu du serveur, mais dans sa version gratuite la résolution maximale est de 1080p. Si tu remarques des fautes ou quelque chose qui ne fonctionne pas, n\u0026rsquo;hésites pas à le mentionner dans les commentaires !\nA bientôt !\n","date":"30 mars 2023","permalink":"/blog/informatique/aws-ec2-cloudgaming-instance/","section":"Blog","summary":"Introduction # Le cloud-gaming se démocratise de plus en plus notamment par le biais d\u0026rsquo;acteurs tel que OVH avec son offre Shadow, ou encore NVDIA avec GeForce Now.","title":"Provisionner sa propre instance de jeu Windows Server dans le cloud AWS via CloudFormation"},{"content":"","date":null,"permalink":"/tags/","section":"Tags","summary":"","title":"Tags"},{"content":"","date":null,"permalink":"/tags/windows-server-2019/","section":"Tags","summary":"","title":"windows-server-2019"},{"content":"","date":null,"permalink":"/tags/appservices/","section":"Tags","summary":"","title":"appservices"},{"content":"","date":null,"permalink":"/tags/azure/","section":"Tags","summary":"","title":"azure"},{"content":"","date":null,"permalink":"/categories/azure/","section":"Categories","summary":"","title":"azure"},{"content":"","date":null,"permalink":"/tags/azuremanagedsql/","section":"Tags","summary":"","title":"azuremanagedsql"},{"content":"Prérequis # Un compte Azure (ici j\u0026rsquo;utilise mon compte Azure Student) Terraform AzureCLI Un compte de stockage Azure, avec un containeur permettant de stocker le terraform.tfstate Dans mon cas, j\u0026rsquo;ai créé un groupe de ressource dédié à mon backend via le portail Azure, puis le compte de stockage dans ce groupe :\nCi ce n\u0026rsquo;est pas déjà fait, il faut t\u0026rsquo;authentifier à Azure via la commande az login et suivre les instructions.\nObjectif # L\u0026rsquo;objectif de cet article est d\u0026rsquo;expliquer la mise en place d\u0026rsquo;une instance GLPI dans le cloud Azure, et de sa base de donnée MySQL en utilisant des services managés proposés par Microsoft. L\u0026rsquo;intégralité du code est disponible ce repo . Cet article est basé sur le déploiement de GLPI, mais devrais fonctionner avec n\u0026rsquo;importe quelle application web fonctionnant avec une base de donnée MySQL.\nInfrastructure cible # Pour déployer notre instance de test, nous allons (dans l\u0026rsquo;ordre) définir :\nUn groupe de ressource Azure qui va contenir l\u0026rsquo;ensemble des ressources Un VPC (Virtual Network sur Azure) 2 subnets : un dédié à la base de données, l\u0026rsquo;autre à l\u0026rsquo;application web. Un serveur managé Azure mysql flexible (plan Bs1) Une base de donnée managée Azure SQL Un plan App Service Linux (B1) Une App Service Linux Une règle de pare-feu autorisant les instances Azure à accéder à la base de donnée. La base de donnée ne sera pas accessible via le web, mais uniquement via notre VPC. En fin de compte, l\u0026rsquo;architecture devrais ressembler à ceci :\nTerraform # Structure # Notre code Terraform se décompose en 3 fichiers :\n├── inputs.tf ├── main.tf └── outputs.tf inputs.tfdéfinit les variables d\u0026rsquo;entrées (adressage, nom des ressources etc). inputs.tf outputs.tfpermet d\u0026rsquo;afficher les données nécessaires pour se connecter à notre app une fois le déploiement terminé. outputs.tf main.tf contient l\u0026rsquo;ensemble du code provisionnant l\u0026rsquo;infrastructure. Je vais le détailler ci-dessous. Configuration du backend # Au début du fichier main.tf, je configure le provider Terraform azurerm, en lui indiquant où stocker le fichier terraform.statepermettant de garder en mémoire la configuration de l\u0026rsquo;infrastructure :\nterraform { backend \u0026#34;azurerm\u0026#34; { resource_group_name = \u0026#34;backend-terraform-rg\u0026#34; storage_account_name = \u0026#34;terraformbackend9809\u0026#34; container_name = \u0026#34;terraform\u0026#34; key = \u0026#34;terraform.tfstate\u0026#34; } required_providers { azurerm = { source = \u0026#34;hashicorp/azurerm\u0026#34; version = \u0026#34;~\u0026gt; 3.47.0\u0026#34; } } required_version = \u0026#34;\u0026gt;= 1.4.0\u0026#34; } provider \u0026#34;azurerm\u0026#34; { features {} } Création du groupe de ressource # Pour créer des ressources dans Azure, il faut créer un groupe de ressources :\n# Resource Group resource \u0026#34;azurerm_resource_group\u0026#34; \u0026#34;rg\u0026#34; { name = var.resource_group_name location = var.location } A noter: le code reprends les variables définies dans le fichier inputs.tf. Il en va de même pour les exemples suivants. Création du VPC et des sous-réseaux # L\u0026rsquo;exemple de code ci-dessous crée un VPC, et deux sous réseaux. Un pour la base de donnée, et un pour l\u0026rsquo;application web :\n# Virtual Network and subnet resource \u0026#34;azurerm_virtual_network\u0026#34; \u0026#34;vnet\u0026#34; { name = var.vnet_name address_space = var.vnet_address_space location = var.location resource_group_name = azurerm_resource_group.rg.name } resource \u0026#34;azurerm_subnet\u0026#34; \u0026#34;mysql_subnet\u0026#34; { name = var.mysql_subnet_name resource_group_name = azurerm_resource_group.rg.name virtual_network_name = azurerm_virtual_network.vnet.name address_prefixes = var.mysql_subnet_address_prefixes service_endpoints = [\u0026#34;Microsoft.Sql\u0026#34;] delegation { name = \u0026#34;vnet-delegation\u0026#34; service_delegation { name = \u0026#34;Microsoft.DBforMySQL/flexibleServers\u0026#34; actions = [ \u0026#34;Microsoft.Network/virtualNetworks/subnets/action\u0026#34; ] } } } resource \u0026#34;azurerm_subnet\u0026#34; \u0026#34;app_service_subnet\u0026#34; { name = var.app_subnet_name resource_group_name = azurerm_resource_group.rg.name virtual_network_name = azurerm_virtual_network.vnet.name address_prefixes = var.app_subnet_address_prefixes delegation { name = \u0026#34;vnet-delegation\u0026#34; service_delegation { name = \u0026#34;Microsoft.Web/serverFarms\u0026#34; actions = [\u0026#34;Microsoft.Network/virtualNetworks/subnets/action\u0026#34;] } } } Les délégations configurées dans les blocs delegationet service_delegationpermettent de designer ces subnets en tant que cibles pour des ressources PaaS Azure. Serveur de base de donnée SQL # J\u0026rsquo;ai choisi de déployer une base de donnée de type flexible, en utilisant le plan B_Standard_B1s. C\u0026rsquo;est suffisant pour notre infrastructure, et peu onéreux. Le tableau des tarifs est disponible ici.\nresource \u0026#34;azurerm_mysql_flexible_server\u0026#34; \u0026#34;mysql\u0026#34; { name = var.mysql_server_name location = azurerm_resource_group.rg.location resource_group_name = azurerm_resource_group.rg.name administrator_login = var.mysql_database_admin_username administrator_password = random_password.mysql_password.result backup_retention_days = 5 sku_name = \u0026#34;B_Standard_B1s\u0026#34; delegated_subnet_id = azurerm_subnet.mysql_subnet.id } Le mot de passe administrateur est généré aléatoirement et sera affiché dans les output après avoir déployé l\u0026rsquo;infrastructure. L\u0026rsquo;option\u0026rsquo; delegated_subnet_id permet d\u0026rsquo;attacher ce serveur au subnet précédement créé.\nBase de donnée SQL # # MySQL Database resource \u0026#34;azurerm_mysql_flexible_database\u0026#34; \u0026#34;mysql\u0026#34; { name = var.mysql_database_name resource_group_name = azurerm_resource_group.rg.name server_name = azurerm_mysql_flexible_server.mysql.name charset = \u0026#34;utf8\u0026#34; collation = \u0026#34;utf8_unicode_ci\u0026#34; } Cet extrait de code crée une base de donnée dédiée à GLPI, sur le serveur MySQL managé Azure.\nRègles de firewall # # Firewall rule resource \u0026#34;azurerm_mysql_flexible_server_firewall_rule\u0026#34; \u0026#34;fw-mysql\u0026#34; { name = \u0026#34;AllowAzureIPs\u0026#34; resource_group_name = azurerm_resource_group.rg.name server_name = azurerm_mysql_flexible_server.mysql.name start_ip_address = \u0026#34;0.0.0.0\u0026#34; end_ip_address = \u0026#34;0.0.0.0\u0026#34; } Par convention, autoriser les ips 0.0.0.0revient à autoriser les ressources Azure uniquement d\u0026rsquo;accéder à la base de donnée.\nPlan Azure App Service et application App Service # resource \u0026#34;azurerm_service_plan\u0026#34; \u0026#34;glpi-service-plan\u0026#34; { name = var.glpi_app_service_plan_name resource_group_name = azurerm_resource_group.rg.name location = azurerm_resource_group.rg.location os_type = \u0026#34;Linux\u0026#34; sku_name = \u0026#34;B1\u0026#34; } resource \u0026#34;azurerm_linux_web_app\u0026#34; \u0026#34;glpi-app-service\u0026#34; { name = var.glpi_app_service_name resource_group_name = azurerm_resource_group.rg.name location = azurerm_service_plan.glpi-service-plan.location service_plan_id = azurerm_service_plan.glpi-service-plan.id site_config { always_on = false application_stack { docker_image = \u0026#34;diouxx/glpi\u0026#34; docker_image_tag = \u0026#34;latest\u0026#34; } } } #Connect the Azure App to subnet resource \u0026#34;azurerm_app_service_virtual_network_swift_connection\u0026#34; \u0026#34;app\u0026#34; { app_service_id = azurerm_linux_web_app.glpi-app-service.id subnet_id = azurerm_subnet.app_service_subnet.id } Cet extrait de code déploye l\u0026rsquo;application web GLPI au sein d\u0026rsquo;un plan B1. Dans mon exemple, j\u0026rsquo;utilise une image docker GLPI hébergée sur le docker hub.\nLe plan B1 est le plan App Service le moins cher permettant d\u0026rsquo;attacher un App Service à un subnet, et ainsi pouvoir communiquer en interne avec la base de donnée. Déploiement de l\u0026rsquo;infrastructure # Une fois le code rédigé (ou téléchargé directement depuis mon repo ), le déploiement s\u0026rsquo;effectue en 3 commandes :\nterraform init terraform plan terraform apply terraform init permet de configurer le backend et récupérer le module azurerm. La commande plan permet de passer en revue les changements apportés à l\u0026rsquo;infrastructure, et apply de la déployer :\nSi tout se passe bien, la sortie de la commande apply te donneras le nécessaire pour configurer ton app GLPI :\nLe déploiement peut prendre du temps, notamment la base de donnée MySQL (+/- 10 minutes dans mon cas). L\u0026rsquo;App Service peut aussi mettre un certain temps avant d\u0026rsquo;être accessible depuis l\u0026rsquo;adresse, le temps que l\u0026rsquo;image docker soit déployée. Dernières étapes de configuration # Dans l\u0026rsquo;interface d\u0026rsquo;installation de GLPI, saisir les informations données dans l\u0026rsquo;output terraform. Si tu as utilisé mon fichier input.tf, l\u0026rsquo;utilisateur SQL sera glpi. La première tentative de connexion donnera cette erreur :\nPour la corriger, il suffit de se rendre dans les paramètres de la BDD sur le portail Azure, et de passer le paramètre require_secure_transportsur OFF:\nIl est normalement désormais possible d\u0026rsquo;installer GLPI sur la base de donnée créée pour l\u0026rsquo;occasion à l\u0026rsquo;aide de Terraform:\nVérifications # Une fois l\u0026rsquo;application installée, on se connecte à l\u0026rsquo;aide des identifiants par défaut (glpi/glpi):\nC\u0026rsquo;est fonctionnel!\nEn raison de la faible puissance de la base de donnée managée choisie, l\u0026rsquo;affichage peut être très long dans l\u0026rsquo;application. Si c\u0026rsquo;est inutilisable, ne pas hésiter à augmenter la taille de l\u0026rsquo;instance SQL. Suppression de l\u0026rsquo;infrastructure # Un simple terraform destroydétruira l\u0026rsquo;ensemble des ressources crées. A ne pas oublier, le coût mensuel de l\u0026rsquo;App Service peut être élevé.\n","date":"13 mars 2023","permalink":"/blog/informatique/azure-appservices-terraform/","section":"Blog","summary":"Prérequis # Un compte Azure (ici j\u0026rsquo;utilise mon compte Azure Student) Terraform AzureCLI Un compte de stockage Azure, avec un containeur permettant de stocker le terraform.","title":"Déployer GLPI 10 sur Azure App Services avec une base de donnée managée via Terraform"},{"content":"","date":null,"permalink":"/tags/glpi/","section":"Tags","summary":"","title":"glpi"},{"content":"","date":null,"permalink":"/tags/iac/","section":"Tags","summary":"","title":"iac"},{"content":"","date":null,"permalink":"/tags/terraform/","section":"Tags","summary":"","title":"terraform"},{"content":"","date":null,"permalink":"/tags/latex/","section":"Tags","summary":"","title":"latex"},{"content":"","date":null,"permalink":"/tags/markdown/","section":"Tags","summary":"","title":"markdown"},{"content":"","date":null,"permalink":"/categories/markdown/","section":"Categories","summary":"","title":"markdown"},{"content":"","date":null,"permalink":"/tags/pandoc/","section":"Tags","summary":"","title":"pandoc"},{"content":"","date":null,"permalink":"/tags/pdf/","section":"Tags","summary":"","title":"pdf"},{"content":"Le language Markdown # Attention Cet article part du principe que vous êtes déjà à l\u0026rsquo;aise avec la rédaction en Markdown. Si ce n\u0026rsquo;est pas le cas, un tutorial interactif assez bien fait est disponible ici Markdown est un language de balisage facile à utiliser et à lire. Je m\u0026rsquo;en suis beaucoup servi lors de mes études pour prendre des notes, car Markdown permet d\u0026rsquo;obtenir un résultat propre très rapidement avec un simple éditeur de texte. Différents logiciels sont compatible avec Markdown, permettant d\u0026rsquo;organiser ses notes et d\u0026rsquo;obtenir un aperçu en temps réel lors de l\u0026rsquo;édition. Personnellement, j\u0026rsquo;utilise Bear sous MacOS:\nInterface principal de Bear, avec le texte formaté en utilisant Markdown Markdown est aussi extrêmement répandu dans le monde de l\u0026rsquo;informatique, la majorité des fichiers README étant rédigé en utilisant la syntaxe Markdown.\nLes limites du Markdown # J\u0026rsquo;ai tellement pris l\u0026rsquo;habitude de rédiger mes documentations et notes en Markdown, que ça soit sur VS Code ou sur des applications type Bear, que l\u0026rsquo;utilisation de Word me paraît désormais extrêmement lourde et contre-productive. Le souci, c\u0026rsquo;est que par défaut, Markdown n\u0026rsquo;est pas prévu pour générer des documents imprimables. En effet, c\u0026rsquo;est uniquement un language de balisage, conçu pour être interprété et affiché à l\u0026rsquo;écran.\nGénérer un document type \u0026ldquo;Mémoire\u0026rdquo; depuis des fichiers Markdown # Je me suis basé sur ce repo pour créer un template (en Français) permettant de générer un document adapté à mes rendus d\u0026rsquo;école. Concrètement, il s\u0026rsquo;agit ici d\u0026rsquo;utiliser Pandoc associé à un template LaTeX permettant de générer le document. Rassurez-vous, il n\u0026rsquo;est pas nécessaire d\u0026rsquo;apprendre LaTeX pour utiliser ce projet, seule certaines commandes suffisent.\nÉtape 1 # Récupérer sur mon GitHub le template :\ngit clone https://github.com/fabienchevalier/phd_thesis_markdown.git Pandoc nécessite LaTeX pour générer des PDF. En fonction de votre OS, il faut donc installer une distribution LaTeX:\nLinux: sudo apt-get install textlive Windows: voir du côté de MiKTex MacOS: via homebrew avec brew install --cask mactex Par la suite, mettre à jour sa distribution LaTeX:\nsudo tlmgr update --self Étape 2 # Installer Pandoc dans un environnement Python séparé. Personnellement, j\u0026rsquo;utilise miniconda sous MacOS (disponible aussi sur Linux/Windows):\nbrew install miniconda #apt-get install miniconda devrais fonctionner sous Ubuntu/Debian conda create -n phd -y python=3.7 pandoc conda activate phd Les autres dépendances peuvent être installées automatiquement via le Makefile proposé :\nmake install Étape 3 # Par la suite, déposer les fichiers md dans le dossier source et executer make pdf pour générer le document au format PDF. Ne pas oublier de modifier le fichier metadata.yml dans le dossier source en fonction des besoins! Un exemple de rendu est disponible ici.\n","date":"7 février 2023","permalink":"/blog/informatique/markdown-to-pdf-latex/","section":"Blog","summary":"Le language Markdown # Attention Cet article part du principe que vous êtes déjà à l\u0026rsquo;aise avec la rédaction en Markdown.","title":"Rédiger son mémoire universitaire en Markdown"},{"content":" Réalisé dans le cadre de mes deux années de BTS, ce portofolio résume les travaux entrepris lors de ces deux années d\u0026rsquo;étude. Entièrement réalisé en Markdown, ce site web est hébergé par GitHub et à été construit à partir du template Jekyll Minimal Mistake.\nhttps://fchevalier.net/bts\n","date":"1 septembre 2022","permalink":"/projets/bts_sio/","section":"Projets","summary":"Réalisé dans le cadre de mes deux années de BTS, ce portofolio résume les travaux entrepris lors de ces deux années d\u0026rsquo;étude.","title":"Portofolio BTS SIO SISR"}]
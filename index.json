[{"content":"Hi ğŸ‘‹ #ğŸ¢ DevOps @Claranet France\nğŸ“‹ Mon CV\n","date":null,"permalink":"/","section":"/home","summary":"","title":"/home"},{"content":" C\u0026rsquo;est ici que je publie mes articles quand le temps me le permet. ğŸ“Œ Articles rÃ©cents #","date":null,"permalink":"/blog/","section":"Blog","summary":"","title":"Blog"},{"content":"","date":null,"permalink":"/categories/","section":"Categories","summary":"","title":"Categories"},{"content":"","date":null,"permalink":"/tags/cloud/","section":"Tags","summary":"","title":"Cloud"},{"content":"","date":null,"permalink":"/tags/dry/","section":"Tags","summary":"","title":"Dry"},{"content":"","date":null,"permalink":"/categories/iac/","section":"Categories","summary":"","title":"Iac"},{"content":"","date":null,"permalink":"/tags/iac/","section":"Tags","summary":"","title":"Iac"},{"content":"","date":null,"permalink":"/tags/","section":"Tags","summary":"","title":"Tags"},{"content":"","date":null,"permalink":"/categories/terraform/","section":"Categories","summary":"","title":"Terraform"},{"content":"","date":null,"permalink":"/tags/terraform/","section":"Tags","summary":"","title":"Terraform"},{"content":"","date":null,"permalink":"/tags/terragrunt/","section":"Tags","summary":"","title":"Terragrunt"},{"content":" Nouvel article! Terragrunt est un wrapper (surcouche) pour Terraform, conÃ§u pour simplifier et optimiser la gestion des configurations d\u0026rsquo;infrastructure en suivant le principe DRY (Don\u0026rsquo;t Repeat Yourself). Dans cet article, je vais tenter d\u0026rsquo;expliquer le fonctionnement de Terragrunt Ã  travers un cas client fictif. Depuis la crÃ©ation du clone open-source de Terraform, Ã  savoir OpenTofu, Terragrunt utilisera par dÃ©faut OpenTofu si il est installÃ© sur ta machine. Dans cet article, je me base sur Terraform mais la logique reste la mÃªme. Introduction #Reprenant les bonnes pratiques issues du monde du dÃ©veloppement, Terragrunt permet de rÃ©duire la duplication de code en factorisant les configurations Terraform. Cet article Ã  pour but d\u0026rsquo;expliquer la mise en place d\u0026rsquo;un projet Terraform avec Terragrunt, et surtout de comprendre lÃ  oÃ¹ Terragrunt apporte de la valeur par rapport Ã  Terraform seul. Attention, Terragrunt nÃ©cessite des bases solides en configurations Terraform, cet article se destine donc Ã  un public ayant dÃ©jÃ  pratiquÃ© Terraform sur des infrastructures moyennes Ã  grandes, et souhaitant en optimiser la gestion afin de passer Ã  l\u0026rsquo;Ã©chelle. Si ce n\u0026rsquo;est pas ton cas, je t\u0026rsquo;invite Ã  consulter cette section du blog de StÃ©phane Robert pour te familiariser avec Terraform. Tout y est trÃ¨s bien expliquÃ©.\nContexte #J\u0026rsquo;ai Ã©tÃ© amenÃ© Ã  travailler pour un client ayant une infrastructure cloud consÃ©quente, et logiquement beaucoup de ressources Ã  gÃ©rer. La principale problÃ©matique rencontrÃ©e dans la gestion d\u0026rsquo;une architecture de cette taille avec Terraform est pour moi l\u0026rsquo;organisation mÃªme du code, et la dette technique qui en dÃ©coule. En effet, Ã  force de multiplier les ressources, variables, environnements etc.. , on peut vite se retrouver avec du code difficile Ã  comprendre et maintenir, surtout lors de phases de build avec des deadlines serrÃ©es.\nTerragrunt se veut Ãªtre une rÃ©ponse efficace Ã  ces problÃ©matiques en apportant une organisation claire et modulaire au code Terraform. En externalisant les paramÃ¨tres spÃ©cifiques Ã  chaque environnement et en automatisant la gestion des dÃ©pendances entre modules, Terragrunt offre une solution structurÃ©e pour minimiser la dette technique.\nIllustration de circonstance : Plan B est un jeu proposant de \u0026hellip; Terraformer une planÃ¨te. Cool, non ? Cependant, son adoption n\u0026rsquo;est pas forcÃ©ment pertinente pour des projets de petite taille, et peut mÃªme Ãªtre contre-productive si mal utilisÃ©e (ğŸ‘‹ Kubernetes). De plus, la courbe d\u0026rsquo;apprentissage peut Ãªtre assez raide, j\u0026rsquo;en ai fais les frais.\nComprendre les concepts de modularitÃ©, dâ€™hÃ©ritage de configurations ou encore de gestion des dÃ©pendances demande un investissement initial non nÃ©gligeable. Cela dit, une fois compris et bien utilisÃ©, Terragrunt peut vite se rÃ©vÃ©ler indispensable. Il permet de structurer efficacement des projets complexes, et de rÃ©duire la duplication de code (factorisation) de maniÃ¨re Ã©lÃ©gante.\nDans cet article, je vais donc tenter de t\u0026rsquo;expliquer une maniÃ¨re d\u0026rsquo;utiliser Terragrunt, Ã  travers un exemple fictif. Je pense qu\u0026rsquo;il est plus facile d\u0026rsquo;intÃ©grer certains concepts appliquÃ©s Ã  une situation concrÃ¨te, plutÃ´t que de se plonger dans la thÃ©orie. AprÃ¨s tout, la doc officielle est (trÃ¨s) bien Ã©crite ğŸ™ƒ.\nJe prÃ©sente ici une mÃ©thode d\u0026rsquo;utilisation de Terragrunt, et non LA mÃ©thode. A tes risques et pÃ©rils ğŸ˜. Les exemples que je vais fournir se basent sur des configurations GCP, mais la logique est applicable Ã  n\u0026rsquo;importe quel cloud provider. Il ne s\u0026rsquo;agira pas ici d\u0026rsquo;expliquer comment bootstrap tel ou tel ressources, mais plutÃ´t de te montrer comment organiser ton code Terraform avec Terragrunt.\nC\u0026rsquo;est quoi concrÃ¨tement, Terragrunt # SchÃ©ma reprÃ©sentatif d\u0026rsquo;une configuration Terragrunt, issu du site de Terragrunt Terragrunt est avant-tout un outil CLI, reprenant la syntaxe de Terraform dans son fonctionnement ad-hoc. On retrouvera donc les fameux terragrunt plan, terragrunt apply, etc.. LÃ  oÃ¹ Ã§a devient intÃ©ressant, c\u0026rsquo;est que Terragrunt propose en sus des fonctionnalitÃ©s comme la gÃ©nÃ©ration dynamiques de fichiers backend.tf (oui, il permet aussi de crÃ©er le bucket Ã  la volÃ©e si celui-ci n\u0026rsquo;existe pas), des fonctions permettant de manipuler des fichiers HCL (comme read_terragrunt_config), ou encore une gestion avancÃ©e des dÃ©pendances entre modules.\nJe te recommande de lire au moins le dÃ©but du quickstart Terragrunt prÃ©sent dans la documentation officielle. Cela t\u0026rsquo;aideras grandement Ã  comprendre les concepts dÃ©taillÃ©s par la suite dans cet article. L\u0026rsquo;organisation du code #L\u0026rsquo;organisation du code, ainsi que l\u0026rsquo;architecture des dossiers sont extrÃªmement importante. Encore une fois, l\u0026rsquo;usage de Terragrunt permet de forcer des bonnes pratiques, mais un mauvais choix d\u0026rsquo;organisation peut vite se rÃ©vÃ©ler cauchemardesque, en particulier avec Terragrunt. Pour cet article, je vais donc choisir de baser mes explications sur le cas d\u0026rsquo;un client fictif, mais qui se rapproche de ce que j\u0026rsquo;ai pu voir dans le monde rÃ©el.\nContexte client #Partons du principe suivant : je souhaite hÃ©berger pour le compte d\u0026rsquo;un client une application 3 tiers classique, afin d\u0026rsquo;hÃ©berger une marketplace. Niveau ressources cloud, on aura donc besoin de :\nJe travaille beaucoup sur GCP en ce moment, pour simplifier la rÃ©daction de mon article, je me base donc sur des assets GCP. Configurer un rÃ©seau VPC (subneting, firewall rules, etc..). Mettre en place une base de donnÃ©es DÃ©ployer une application web (front + back), dans mon exemple containÃ©risÃ©e Mettre en place un load balancer et un orchestrateur de conteneurs, par facilitÃ©e je vais ici utiliser CloudRun. On parle d\u0026rsquo;IaC, il va donc falloir crÃ©er et organiser son code via des repos Git. Les dÃ©veloppeurs vont travailler sur leurs repos respectifs, Ã  savoir marketplace-frontend et marketplace-backend. En parallÃ¨le, il va falloir crÃ©er un repo appelÃ© marketplace-infrastructure permettant de dÃ©finir l\u0026rsquo;infrastructure permettant d\u0026rsquo;hÃ©berger l\u0026rsquo;application (base de donnÃ©es, load balancer, etc..). Enfin, un repo infrastructure-shared-modules sera crÃ©Ã© afin de gÃ©rer et versionner nos modules. Je ne vais pas m\u0026rsquo;attarder sur les questions de CI/CD et autres, ce n\u0026rsquo;est pas le sujet ici.\nJe ne fournirais pas le code des modules, ni mÃªme la configuration complÃ¨te de l\u0026rsquo;infrastructure. Je me limite ici uniquement Ã  l\u0026rsquo;abstraction Terragrunt. J\u0026rsquo;ai cependant rÃ©digÃ© un article dÃ©taillant la mise en place complÃ¨te d\u0026rsquo;une infrastructure via Terraform disponible ici. SchÃ©ma d\u0026rsquo;architecture #Pour contextualiser un peu tout Ã§a, voici un schÃ©ma d\u0026rsquo;architecture basique reprÃ©sentant les briques Ã  dÃ©ployer :\nSchÃ©ma de l\u0026rsquo;architecture de l\u0026rsquo;application D\u0026rsquo;un premier abord, dans le cas oÃ¹ mon client fictif ne souhaite utiliser que deux environnements de dÃ©veloppement, pour ce projet unique, Terragrunt serais overkill. Imaginons maintenant que ce mÃªme client souhaite 4 environnements, Ã  savoir dev, staging, preprod et prod. De plus, il viens de recevoir une demande du mÃ©tier, nÃ©cessitant la mise en place de 6 applications similaires. Tu vois ou je veux en venir ?\nHiÃ©rarchie des dossiers # La logique d\u0026rsquo;organisation des dossiers est directement inspirÃ©e de l\u0026rsquo;excellent repo maintenu par Padok, fournissant des bonnes pratiques sur l\u0026rsquo;utilisation de Terraform, et notamment le concept de layers. Je t\u0026rsquo;invites Ã  le consulter. Je sais donc que je dois livrer la marketplace en premier lieu, sur 4 environnements, mais avec en tÃªte le fait que quelques semaines plus tard la demande Ã©voluera. LÃ , on rentre dans un cas ou Terragrunt pourra m\u0026rsquo;Ãªtre utile, car je vais pouvoir factoriser dÃ¨s le dÃ©but.\nPour illustrer mon explication, je vais me concentrer sur deux briques de l\u0026rsquo;architecture, Ã  savoir le load balancer et le cloud run. Bien sÃ»r, il faudra configurer bien plus de ressources afin de rendre mon architecture fonctionnelle, mais je cherche ici Ã  expliquer la logique de factorisation induite par Terragrunt.\nLe repo marketplace-infrastructure #Voici l\u0026rsquo;architecture de dossier proposÃ©e :\nâ””â”€â”€ layers â”œâ”€â”€ cloud-run â”‚ â”œâ”€â”€ marketplace-frontend | | â”œâ”€â”€ dev | | â”œâ”€â”€ staging | | â”œâ”€â”€ preprod | | â””â”€â”€ prod | |â”€â”€ marketplace-backend | â”œâ”€â”€ dev | â”œâ”€â”€ staging | â”œâ”€â”€ preprod | â””â”€â”€ prod â””â”€â”€ load-balancer â”œâ”€â”€ dev â”œâ”€â”€ staging â”œâ”€â”€ preprod â””â”€â”€ prod etc etc... Ma ressource cloud-run permettra de dÃ©ployer les deux services de mon application, Ã  savoir le frontend et le backend. Je vais donc crÃ©er un dossier cloud-run, contenant deux sous-dossiers, un pour chaque service. Chaque service contiendra les environnements de dÃ©veloppement, staging, preprod et prod.\nLe dossier load-balancer contiendra lui aussi les environnements de dÃ©veloppement, staging, preprod et prod.\nLe repo infrastructure-shared-modules #Le mot-clÃ© factorisation reviens souvent dans cet article. On a parlÃ© plus haut d\u0026rsquo;un futur besoin pour mon client d\u0026rsquo;ajouter de nouvelles applications. Au lieu d\u0026rsquo;avoir Ã  rÃ©Ã©crire (ou copier/coller) le code de chaque module, lors de la crÃ©ation d\u0026rsquo;une nouvelle application, autant tout rassembler au mÃªme endroit dans un repo sÃ©parÃ©. Je le hiÃ©rarchise de cette maniÃ¨re :\nâ””â”€â”€ modules â”œâ”€â”€ cloud-run â”œâ”€â”€ load-balancer â”œâ”€â”€ vpc etc etc... Ok, nous avons Ã  prÃ©sent nos deux repo, ainsi qu\u0026rsquo;une architecture de dossier claire et facile Ã  comprendre. Passons aux fichiers de configuration.\nFichiers de configurations Terragrunt #Au sein du dossier layers, dans le repo marketplace-infrastructure Ã  la racine, je vais crÃ©er un fichier root.hcl (le fichier de configuration Terragrunt racine).\nTerragrunt fonctionne de maniÃ¨re rÃ©cursive. Si tu appliques la configuration Terragrunt du dossier layers/load-balancer/dev/terragrunt.hcl, il va remonter dans l\u0026rsquo;arborescence jusqu\u0026rsquo;Ã  trouver le fichier root.hcl et l\u0026rsquo;injecter dans la configuration. Le nommage est ici est complÃ¨tement arbitraire, mais sache que Terragrunt Ã  besoin d\u0026rsquo;au moins un fichier appelÃ© terragrunt.hcl Ã  la racine de chacune de tes stacks.\nLe concept de stack dans Terragrunt reprÃ©sente une unitÃ© de dÃ©ploiement. Chaque stack correspond Ã  un environnement spÃ©cifique (par exemple, dev, staging, prod) et peut contenir plusieurs modules ou ressources. En d\u0026rsquo;autres termes, une stack est une collection de ressources Terraform qui sont gÃ©rÃ©es ensemble. Fonctions Terragrunt #Terragrunt propose un certain nombre de fonctions permettant de manipuler les fichiers de configuration, de la mÃªme maniÃ¨re que les fonctions natives Terraform. J\u0026rsquo;en utilise quelques unes dans cet article. Sans aller dans le dÃ©tail, Terragrunt propose des fonctions supplÃ©mentaires, plus globales permettant de rÃ©cupÃ©rer du contexte de faÃ§on dynamique. Des exemples sont donnÃ©s et expliquÃ©s dans les sections suivantes. Tu peux retrouver l\u0026rsquo;ensemble de ces fonctions documentÃ©es ici : Terragrunt Built-in Functions.\nConfiguration des locals #Dans root.hcl, on va dÃ©finir dÃ©finir des locals, c\u0026rsquo;est Ã  dire des expressions communes Ã  l\u0026rsquo;ensemble de mes environnements dans un contexte Terraform. On y retrouvera donc l\u0026rsquo;id du projet, la region, et autres. Voici un exemple :\n# root.hcl locals { env_name = basename(get_original_terragrunt_dir()) region = \u0026#34;europe-west2\u0026#34; env_config = lookup(local.env_settings, local.env_config, {}) env_settings = { dev = { project_id = \u0026#34;marketplace-dev\u0026#34; vpc = \u0026#34;marketplace\u0026#34; labels = { environment = \u0026#34;dev\u0026#34; application = \u0026#34;marketplace\u0026#34; managed_by = \u0026#34;terraform\u0026#34; } }, staging = { project_id = \u0026#34;marketplace-staging\u0026#34; vpc = \u0026#34;marketplace\u0026#34; labels = { environment = \u0026#34;staging\u0026#34; application = \u0026#34;marketplace\u0026#34; managed_by = \u0026#34;terraform\u0026#34; } }, preprod = { project_id = \u0026#34;marketplace-preprod\u0026#34; vpc = \u0026#34;marketplace\u0026#34; labels = { environment = \u0026#34;preprod\u0026#34; application = \u0026#34;marketplace\u0026#34; managed_by = \u0026#34;terraform\u0026#34; } }, prod = { project_id = \u0026#34;marketplace-prod\u0026#34; vpc = \u0026#34;marketplace\u0026#34; labels = { environment = \u0026#34;prod\u0026#34; application = \u0026#34;marketplace\u0026#34; managed_by = \u0026#34;terraform\u0026#34; } } } } inputs = {} Dans le cas ou l\u0026rsquo;on souhaite utiliser un Cloud Provider different, comme AWS par exemple, il faudra adapter la configuration des locals en fonction de la logique de nommage de ton provider. Par exemple, pour AWS, il faudra adapter le region et le project_id en fonction de la logique de nommage AWS (account_id par exemple au lieu du project_id). Ce fichier de configuration part du principe qu\u0026rsquo;un rÃ©seau VPC a dÃ©jÃ  Ã©tÃ© crÃ©Ã© par projet. Ici, dans chaque environnement, on connectera les assets au rÃ©seau marketplace. DÃ©taillons un peu ce fichier :\nenv_name: ici, la fonction basename() permet d\u0026rsquo;extraire le dernier segment d\u0026rsquo;un chemin donnÃ©. UtilisÃ©e de pair avec get_original_terragrunt_dir(), elle permet d\u0026rsquo;obtenir le nom de l\u0026rsquo;environnement actuel (par exemple, dev, staging, etc.) en se basant sur le chemin du rÃ©pertoire oÃ¹ se situe terragrunt.hcl, toujours de maniÃ¨re absolue.\nregion: trÃ¨s simple, ici la rÃ©gion est hardcodÃ©e car ne changera pas.\nenv_config : utilise lookup() (fonction Terraform) pour rÃ©cupÃ©rer la configuration de lâ€™environnement actuel. Si lâ€™environnement nâ€™existe pas dans env_settings, la valeur par dÃ©faut {} est utilisÃ©e pour Ã©viter les erreurs.\nenv_settings: ce bloc dÃ©finit une map contenant les configurations spÃ©cifiques Ã  chaque environnement (par exemple, dev, staging, etc.). Ici, on l\u0026rsquo;utilise pour spÃ©cifier de maniÃ¨re globale le project_id et le vpc pour chaque environnement (dans l\u0026rsquo;exemple donnÃ©). On pourrait y ajouter d\u0026rsquo;autres configurations que l\u0026rsquo;on souhaite centraliser tel que des tags etc.\nLes fonctions lookup() et basename() sont des fonctions natives de Terraform. Terragrunt n\u0026rsquo;Ã©tant qu\u0026rsquo;une surcouche de Terraform, il est possible d\u0026rsquo;utiliser ces fonctions dans les fichiers de configuration Terragrunt. Au final, malgrÃ© la complexitÃ© apparente, ce fichier root.hcl ne fais que fournir des variables gÃ©nÃ©riques de faÃ§on dynamique.\nGestion du backend Terraform #A la suite de la dÃ©claration des locals, on va pouvoir dÃ©finir la configuration du backend Terraform. En effet, Terragrunt permet de gÃ©rer le backend de maniÃ¨re centralisÃ©e, et de le gÃ©nÃ©rer automatiquement dans chaque layer. Voici un exemple de configuration :\nremote_state { backend = \u0026#34;gcs\u0026#34; # Google Cloud Storage, mais peut Ãªtre S3, etc. config = { encrypt = true bucket = \u0026#34;${local.project_id}-tfstates\u0026#34; project = local.project_id prefix = \u0026#34;tfstate/${path_relative_to_include()}\u0026#34; location = local.region } generate = { path = \u0026#34;backend.tf\u0026#34; if_exists = \u0026#34;skip\u0026#34; } } On va donc se retrouver avec un fichier state par environnement, et par module. Par exemple, pour le module cloud-run, on va se retrouver avec un fichier tfstate dans le bucket marketplace-dev-tfstates (pour l\u0026rsquo;environnement dev), avec le prefix tfstate/cloud-run/marketplace-frontend. Terragrunt propose aussi de gÃ©nÃ©rer le fichier provider.tf, de la mÃªme maniÃ¨re que le fichier backend.tf. Dans mon exemple, je prÃ©fÃ¨re gÃ©rer la configuration du provider dans chaque module, mais sache cela reste une possibilitÃ©.\nConfiguration d\u0026rsquo;un layer #Pour le moment donc, j\u0026rsquo;ai couvert la configuration racine, permettant de factoriser Ã  la base de l\u0026rsquo;arborescence. Comme dit plus haut, c\u0026rsquo;est de cette maniÃ¨re que Terragrunt permet de construire une infrastructure en mode DRY. Descendons maintenant d\u0026rsquo;un cran dans l\u0026rsquo;arborescence, et voyons comment configurer un asset cloud-run par exemple.\nGestion des modules Terraform de faÃ§on DRY #Naviguons dans le dossier layers/cloud-run/. Son contenu sera le suivant :\nâ””â”€â”€ cloud-run â”œâ”€â”€ dev â”‚ â”œâ”€â”€ inputs.hcl â”‚ â””â”€â”€ terragrunt.hcl â”œâ”€â”€ module.hcl â”œâ”€â”€ preprod â”‚ â”œâ”€â”€ inputs.hcl â”‚ â””â”€â”€ terragrunt.hcl â”œâ”€â”€ prod â”‚ â”œâ”€â”€ inputs.hcl â”‚ â””â”€â”€ terragrunt.hcl â””â”€â”€ staging â”œâ”€â”€ inputs.hcl â””â”€â”€ terragrunt.hcl Dans l\u0026rsquo;exemple fourni par Gruntwork, tout est centralisÃ© au sein d\u0026rsquo;un fichier unique terragrunt.hcl. En crÃ©ant un fichier inputs, je me permet de sÃ©parer la logique de configuration de la logique d\u0026rsquo;include. C\u0026rsquo;est un choix arbitraire, mais qui me semble plus clair. MÃªme chose pour le fichier module.hcl, qui source le module Terraform. CommenÃ§ons par le contenu du fichier module.hcl. Rappelles toi, j\u0026rsquo;ai indiquÃ© en intro qu\u0026rsquo;il Ã©tait nÃ©cessaire de crÃ©er un repo infrastructure-shared-modules pour y stocker mes modules. C\u0026rsquo;est ici que je vais les rÃ©fÃ©rencer :\n# module.hcl terraform { source = \u0026#34;git@github.com:my-org/infrastructure-shared-modules.git//modules/cloudrun?ref=cloudrun-v1.0.0\u0026#34; } Ici, je fais un choix. Je pourrais sourcer directement le module dans chaque terragrunt.hcl, afin d\u0026rsquo;Ãªtre en mesure d\u0026rsquo;avoir un module pour chaque environnement. Cela-dit, dans une logique qui se veut ISO, je prÃ©fÃ¨res que chaque environnements soient identiques Ã  la production. ConcrÃ¨tement, mon repo infrastructure-shared-modules va contenir l\u0026rsquo;ensemble de mes modules, versionnÃ©s via git.\nJe te conseilles vivement de tag tes modules, afin de pouvoir revenir en arriÃ¨re si besoin. En effet, si tu fais Ã©voluer un module, et que tu ne tag pas ta version, tu risques d\u0026rsquo;appliquer les changements Ã  l\u0026rsquo;ensemble des configurations. En backstage, Terragrunt va cloner ce repo, et lui fournir les inputs dÃ©finies dans chaque fichiers inputs.hcl. On retrouvera d\u0026rsquo;ailleurs un dossier .terragrunt-cache, contenant le repo clonÃ© dans chaque environnement.\nQue se passe-t-il lorsque je lance un terragrunt apply ? #Un petit coup d\u0026rsquo;oeil sur le fichier terragrunt.hcl permet de se rendre compte de toute la logique expliquÃ©e prÃ©cÃ©demment :\ninclude \u0026#34;root\u0026#34; { path = find_in_parent_folders(\u0026#34;root.hcl\u0026#34;) merge_strategy = \u0026#34;deep\u0026#34; } include \u0026#34;module\u0026#34; { path = find_in_parent_folders(\u0026#34;module.hcl\u0026#34;) merge_strategy = \u0026#34;deep\u0026#34; } include \u0026#34;inputs\u0026#34; { path = \u0026#34;inputs.hcl\u0026#34; merge_strategy = \u0026#34;deep\u0026#34; } Ce qui se rÃ©sume schÃ©matiquement :\nflowchart TD A[terragrunt.hcl] --\u003e B[root.hcl] A --\u003e C[module.hcl] A --\u003e D[inputs.hcl] Terragrunt parcourra depuis le dossier courant, et remontera dans l\u0026rsquo;arborescence jusqu\u0026rsquo;Ã  trouver les fichiers root.hcl; module.hcl et inputs.hcl. Il va ensuite fusionner le contenu de ces fichiers, en appliquant la stratÃ©gie de fusion dÃ©finie dans chaque include. Dans notre cas, on a choisi la stratÃ©gie deep, qui va permettre de fusionner les objets imbriquÃ©s. La documentation de Terragrunt permettra d\u0026rsquo;en savoir plus sur les diffÃ©rentes stratÃ©gies de fusion, et le fonctionnement d\u0026rsquo;include en gÃ©nÃ©ral.\nPour faire simple, on peut partir du principe que Terragrunt va fusionner les fichiers de configuration en un seul fichier, en appliquant la stratÃ©gie de fusion dÃ©finie dans chaque include. Il va ensuite appliquer cette configuration au module Terraform. Le fichier input.hcl contiendra donc l\u0026rsquo;Ã©quivalent d\u0026rsquo;un fichier variables.tf dans un module Terraform classique. Il va permettre de dÃ©finir les variables d\u0026rsquo;entrÃ©es pour le module, et sera fusionnÃ© avec les autres fichiers de configuration. Le concept de `terragrunt run-all #Cette commande va permettre d\u0026rsquo;appliquer l\u0026rsquo;ensemble des configurations de maniÃ¨re rÃ©cursive, en une seule commande. C\u0026rsquo;est bien lÃ  que toute la puissance de Terragrunt se rÃ©vÃ¨le.\nterragrunt run-all apply layers/ En effet, si tu as bien suivi, cette commande permettra en une fois de crÃ©er l\u0026rsquo;ensemble des ressources de l\u0026rsquo;application, pour chaque environnement, et d\u0026rsquo;automatiquement gÃ©nÃ©rer la configuration backend.tf.\nPour peu que tu aie les droits sur ton cloud-provider, Terragrunt se charge mÃªme de crÃ©er le bucket tfstates pour toi, si celui-ci n\u0026rsquo;existe pas. DRY. Il est prÃ©fÃ©rable, dans un contexte ou des dÃ©pendances sont prÃ©sentes, de passer par run-all afin que Terragrunt puisse correctement mettre Ã  jour l\u0026rsquo;Ã©tat des ressources. En effet, si tu appliques les configurations une par une, certaines valeurs peuvent Ãªtre obsolÃ¨tes (ex. connection_string d\u0026rsquo;une base de donnÃ©es), et donc entraÃ®ner des erreurs lors de l\u0026rsquo;application des configurations. Last but not least : la gestion des dÃ©pendances #Tu t\u0026rsquo;es peut Ãªtre posÃ© la question Ã  le lecture de cet article : si Terragrunt cherche Ã  appliquer une configuration dÃ©pendante d\u0026rsquo;une autre, comment fait-il pour savoir dans quel ordre appliquer les ressources ? En effet, si je souhaite crÃ©er un cloud-run et lui permettre de se connecter Ã  une base de donnÃ©es, il faut que la base de donnÃ©es soit crÃ©Ã©e avant le cloud-run. Terragrunt propose une fonctionnalitÃ© permettant de gÃ©rer les dÃ©pendances entre modules, et donc d\u0026rsquo;appliquer les ressources dans le bon ordre.\nAttention, Terragrunt reste un wrapper Terraform. Pour que le systÃ¨me de dÃ©pendances fonctionne, il faut que tes modules Terraform puissent output des valeurs que tu pourras ensuite utiliser dans d\u0026rsquo;autres modules. Par exemple, si tu souhaites crÃ©er un cloud-run qui se connecte Ã  une base de donnÃ©es, il faut que le module de la base de donnÃ©es output la connection_string de la base de donnÃ©es, afin que le module cloud-run puisse s\u0026rsquo;y connecter. ConcrÃ¨tement, cela peut donner quelque chose comme Ã§a pour un cloud-run ayant besoin d\u0026rsquo;exposer une variable d\u0026rsquo;environnement DATABASE_URL :\n# cloud-run/inputs.hcl dependency \u0026#34;cloud_sql\u0026#34; { config_path = \u0026#34;cloud-sql/${basename(get_original_terragrunt_dir())}\u0026#34; mock_outputs = { connection_string = \u0026#34;postgres://user:password@host:port/dbname\u0026#34; } } locals { # On rÃ©cupÃ¨re la configuration commune dÃ©finie dans root.hcl common = read_terragrunt_config(find_in_parent_folders(\u0026#34;root.hcl\u0026#34;)).locals config = local.common.config } inputs = { project_id = local.config.project_id region = local.common.region env_name = local.common.env_name secrets = { DATABASE_URL = { value = dependency.cloud_sql.outputs.connection_string } } } Attardons nous un peu sur la partie mock_output. C\u0026rsquo;est une des fonctionnalitÃ©s Terragrunt qui peuvent se rÃ©vÃ©ler intÃ©ressantes pour le dÃ©veloppement. En effet, si tu souhaites tester ta configuration sans avoir Ã  crÃ©er l\u0026rsquo;ensemble des ressources, tu peux utiliser mock_output pour simuler les valeurs de sortie d\u0026rsquo;un module. Cela te permet de plan sans obtenir d\u0026rsquo;erreur lorsque le module n\u0026rsquo;est pas encore crÃ©Ã©.\nPour conclure #ConcrÃ¨tement, la difficultÃ© se situe vraiment dans la comprÃ©hension de la logique de merge de Terragrunt. Pour maintenir une infrastructure DRY, Terragrunt utilise des fonctions et des includes pour fusionner les fichiers de configuration Ã  l\u0026rsquo;apply. Cela nÃ©cessite donc (comme souvent lorsqu\u0026rsquo;on automatise) de rendre dynamique la configuration, et de recourir Ã  des fonctions comme get_parent_terragrunt_dir() ou get_original_terragrunt_dir() afin de variabiliser un maximum.\nTu l\u0026rsquo;as bien compris, la mise en place d\u0026rsquo;une architecture DRY avec Terragrunt reste relativement complexe de part l\u0026rsquo;abstraction qu\u0026rsquo;elle propose. Selon moi, la valeur ajoutÃ©e vaut le coup si :\nTon projet possÃ¨de de nombreux environnements Ã  gÃ©rer (dev, staging, prod, etc..). Tu dois gÃ©rer un grand nombre d\u0026rsquo;applications/infrastructures identiques, mais dont la configuration diffÃ¨re par environnement (une BDD plus petite en dev\u0026hellip;). Tu souhaites gÃ©rer des modules Terraform de maniÃ¨re centralisÃ©e, et versionnÃ©e, et les appliquer Ã  l\u0026rsquo;ensemble de tes environnements. Terragrunt devrais Ãªtre un gros no-go si :\nTon projet est de petite taille, et que tu n\u0026rsquo;as pas besoin de gÃ©rer plus de deux environnements. Tu n\u0026rsquo;as pas forcÃ©ment la bande passante et/ou une Ã©quipe qui te permettra de maintenir tes modules. Tu n\u0026rsquo;est pas 100% Ã  l\u0026rsquo;aise avec Terraform : pour dÃ©bugger du Terragrunt il faut Ãªtre en mesure de rÃ©ellement comprendre ce qu\u0026rsquo;il applique en arriÃ¨re plan. Cet article dÃ©cris l\u0026rsquo;organisation et la maniÃ¨re de gÃ©rer l\u0026rsquo;infrastructure via les commandes ad-hoc Terragrunt (terragrunt plan/apply) afin d\u0026rsquo;en cerner le fonctionnement. Bien entendu, il est possible d\u0026rsquo;utiliser Terragrunt dans un pipeline CI/CD. Une action managÃ©e GitHub et d\u0026rsquo;ailleurs proposÃ©e par Gruntwork, l\u0026rsquo;Ã©diteur de Terragrunt.\nN\u0026rsquo;hÃ©sites pas Ã  me contacter ou de laisser un commentaire si tu souhaites Ã©changer sur le sujet, j\u0026rsquo;ai l\u0026rsquo;impression que Terragrunt commence Ã  avoir le vent en poupe, et je serais ravi d\u0026rsquo;en discuter avec toi.\nSources # Terragrunt Documentation Terraform Documentation terragrunt-infrastructure-live-example docs-terraform-guidelines Reduce redundancy in your Terraform code with Terragrunt ","date":"16 avril 2025","permalink":"/blog/informatique/terragrunt-terraform-dry/","section":"Blog","summary":"DÃ©couverte du wrapper Terragrunt, permettant de gÃ©rer des configurations Terraform en adoptant une approche DRY (Don\u0026rsquo;t Repeat Yourself).","title":"Terragrunt et dÃ©couverte de l'IAC en mode DRY"},{"content":" Ce projet Ã  Ã©tÃ© rÃ©alisÃ© dans le cadre de ma premiÃ¨re annÃ©e de Master, en collaboration avec Matteoz. Je me suis tout particuliÃ¨rement intÃ©ressÃ© Ã  la partie DevOps du projet, Ã  savoir la mise en place d\u0026rsquo;une pipeline de livraison continue ainsi que la partie infrastructure as code permettant d\u0026rsquo;hÃ©berger l\u0026rsquo;applicatif. Introduction #Le but de cet article est de rÃ©sumer la faÃ§on dont j\u0026rsquo;ai apprÃ©hendÃ© l\u0026rsquo;intÃ©gration d\u0026rsquo;un applicatif au sein d\u0026rsquo;une infrastructure dite cloud-native. Ce projet couvrais le dÃ©veloppement d\u0026rsquo;une API REST, jusqu\u0026rsquo;Ã  son dÃ©ploiement de faÃ§on continue sur un environnement AWS imposÃ©, Ã  savoir AWS ECS. S\u0026rsquo;agissant d\u0026rsquo;un travail en groupe de deux, la rÃ©partition de la charge de travail Ã  Ã©tÃ© Ã©tablie comme ceci :\nMatteoz c\u0026rsquo;est chargÃ© de la partie dÃ©veloppement applicative Pour ma part, j\u0026rsquo;ai mis en place une usine logicielle permettant Ã  mon collÃ¨gue de dÃ©ployer l\u0026rsquo;application sur une infrastructure managÃ©e par le code. La documentation du projet est aussi industrialisÃ©e et rÃ©digÃ©e en Markdown puis poussÃ©e par Material for MkDocs. L\u0026rsquo;ensemble du code source et la documentation projet est disponible sur le groupe GitLab du projet.\nCet article n\u0026rsquo;as pas pour but de faire doublon avec la doc technique du projet. Il s\u0026rsquo;agit ici de rÃ©sumer dans les grandes lignes la faÃ§on dont j\u0026rsquo;ai apprÃ©hendÃ© la rÃ©alisation de ce projet ProblÃ©matique #Comme c\u0026rsquo;est le cas dans une situation pro, le sujet fourni est une forme d\u0026rsquo;expression des besoins ayant pour finalitÃ© de rÃ©pondre Ã  une problÃ©matique. Le contexte est donc le suivant :\nUne sociÃ©tÃ© appelÃ©e Solution Libre propose des services internes et externes pour lesquels les problÃ©matiques de chiffrement sont essentiels. Pour rÃ©pondre Ã  ses besoins en la matiÃ¨re, une API de gestion de certificats doit Ãªtre dÃ©veloppÃ©e. Les fonctionnalitÃ©s attendues sont indiquÃ©es dans le sujet Ã  savoir :\nUn CRUD permettant la gestion des droits La gÃ©nÃ©ration de certificats Leur gestion (CRUD) Leur vÃ©rification (y compris les certificats externes) CÃ´tÃ© tooling, certaines choses sont imposÃ©es :\nGolang pour le language de programmation GitLab pour l\u0026rsquo;hÃ©bergement de code source + CI/CD AWS ECS pour la partie hÃ©bergement La solution fournie doit Ãªtre livrÃ©e sous forme de microservice Un cas DevOps concret #Une dimension essentielle de mon mÃ©tier consiste Ã  permettre Ã  une Ã©quipe de dÃ©veloppeurs de dÃ©ployer leurs applications sans se prÃ©occuper des processus de dÃ©ploiement. Ce projet sert donc d\u0026rsquo;exemple pratique pour illustrer l\u0026rsquo;intÃ©gration d\u0026rsquo;une application au sein d\u0026rsquo;une infrastructure web.\nComprÃ©hension des enjeux #Ma premiÃ¨re rÃ©flexion, avant mÃªme de rÃ©flÃ©chir d\u0026rsquo;un point de vue technique c\u0026rsquo;est de comprendre ce que je dois intÃ©grer. Ici, il s\u0026rsquo;agit d\u0026rsquo;une API destinÃ©e Ã  Ãªtre utilisÃ©e en interne. L\u0026rsquo;API ne sera donc pas publique. Le dÃ©veloppeur travaille sur ce que l\u0026rsquo;on pourrais dÃ©finir comme un backend, rÃ©alisÃ© en Go. Il s\u0026rsquo;appuie sur le framework PocketBase pour la gestion de l\u0026rsquo;authentification. PocketBase embarque une base de donnÃ©e SQLite intÃ©grÃ©e lui permettant de stocker les users et roles. Pour stocker les certificats gÃ©nÃ©rÃ©s par l\u0026rsquo;application, l\u0026rsquo;API se repose sur une base de donnÃ©e PostgreSQL. Il souhaiterais proposer un systÃ¨me de notification permettant d\u0026rsquo;avertir lorsqu\u0026rsquo;un certificat arrive Ã  expiration. Bien sÃ»r, je me dois de proposer un systÃ¨me permettant au dÃ©veloppeur de dÃ©ployer son application de faÃ§on la plus simple possible.\nTraduction des enjeux en faisabilitÃ© technique # Certaines briques techniques et choix relatifs Ã  l\u0026rsquo;infrastructure sont imposÃ©s par le sujet. Cependant, pour garder une logique dans la rÃ©daction de cet article, je part du principe que j\u0026rsquo;ai moi mÃªme arbitrÃ© ces choix et vais donc argumenter en ce sens. Mon application est une API interne #Cette API ne doit pas Ãªtre accessible publiquement. Je propose donc :\nD\u0026rsquo;hÃ©berger cette application dans un subnet privÃ©, au sein du VPC de l\u0026rsquo;entreprise. Je met donc en place un bastion permettant de requÃªter l\u0026rsquo;API lors de la phase de dÃ©bogage. L\u0026rsquo;accÃ¨s Ã  ce bastion sera restreint aux seules personnes autorisÃ©es. Mon application est dÃ©veloppÃ©e en Go #L\u0026rsquo;Ã©quipe de dÃ©veloppement me fournit un Dockerfile, me permettant de construire une image qui pourra ensuite Ãªtre dÃ©ployÃ©e en production. Pour ce faire :\nJe crÃ©e un cluster ECS, et prÃ©pare un service permettant d\u0026rsquo;hÃ©berger l\u0026rsquo;image ainsi construite. Pour faciliter un maximum la maintenance, je me base sur une architecture serverless proposÃ©e par AWS : Fargate. Mon application se base sur un middleware d\u0026rsquo;authentification particulier : PocketBase #Je me documente sur la maniÃ¨re dont PocketBase fonctionne. D\u0026rsquo;un point de vue infrastructure, il faut rÃ©flÃ©chir Ã  la rÃ©silience des donnÃ©es. Premier problÃ¨me :\nPocketBase ne permet pas d\u0026rsquo;utiliser une base de donnÃ©e externe, et intÃ¨gre SQLite pour le stockage des users et roles. Plusieurs solutions :\nMonter un volume EFS afin d\u0026rsquo;Ãªtre en mesure de faire des sauvegardes AprÃ¨s plusieurs recherches, utiliser SQLite sur un partage rÃ©seau semble ne pas fonctionner comme voulu.\nPasser par une solution logicielle : LiteStream. LiteStream permet tout simplement de rÃ©pliquer en temps rÃ©el la base de donnÃ©e SQLite vers par exemple, un bucket S3.\nMon application Ã  besoin d\u0026rsquo;une base de donnÃ©e PostgreSQL #Je propose d\u0026rsquo;utiliser une solution managÃ©e AWS : Aurora\nL\u0026rsquo;Ã©quipe de dÃ©veloppeur souhaite mettre en place un systÃ¨me de notification #DiffÃ©rents services de communication (Slack, Discord, Teams\u0026hellip;) proposent des intÃ©grations sous la forme de WebHooks. Dans notre cas, nous avons choisi la solution Discord pour prÃ©senter un POC. La feature que j\u0026rsquo;ai proposÃ©e est disponible dans ce commit.\nL\u0026rsquo;Ã©quipe de dÃ©veloppeur doit Ãªtre autonome dans ses dÃ©ploiements #Mon infrastructure sera construite as code, et je fournirais des pipelines de dÃ©ploiement permettant Ã  l\u0026rsquo;Ã©quipe de dÃ©ployer facilement ses versions. Le tout Ã©tant hÃ©bergÃ© sur GitLab, je me baserais sur AutoDevops dans la mesure du possible, afin de ne pas avoir Ã  maintenir des pipelines complexes. De plus, GitLab propose des outils pratiques permettant par exemple de gÃ©rer les backends Terraform. Enfin, j\u0026rsquo;ai appris que rÃ©cemment Terraform change de license. Je choisis d\u0026rsquo;utiliser un fork open-source pour gÃ©rer mon infrastructure : OpenTofu.\nBien faire attention aux licenses des outils que l\u0026rsquo;on utilise. Bien que la majoritÃ© des outils DevOps soient open-source, cela peut Ãªtre amenÃ© Ã  changer et poser quelques problÃ©matiques lorsqu\u0026rsquo;on propose des services commerciaux basÃ©s sur ces outils. Schema d\u0026rsquo;architecture #La meilleure maniÃ¨re de poser des bases concrÃ¨tes quand on parle d\u0026rsquo;architecture informatique, c\u0026rsquo;est de rÃ©aliser un schÃ©ma. Le voici :\nCe schema reprÃ©sente de faÃ§on graphique l\u0026rsquo;ensemble des points Ã©voquÃ©s ci-dessus L\u0026rsquo;ensemble du code d\u0026rsquo;infrastructure est disponible sur le repo GitLab du projet.\nSpÃ©cificitÃ©s liÃ©es Ã  l\u0026rsquo;environnement du projet #Nous avons lors de la phase de rÃ©alisation allÃ¨grement dÃ©passÃ© la limite offerte sur les runners GitLab sur sa version Free. De plus, l\u0026rsquo;environnement AWS fourni par l\u0026rsquo;Ã©cole comportais certaines restrictions, notamment l\u0026rsquo;impossibilitÃ© de configurer les rÃ´les IAM sur l\u0026rsquo;environnement proposÃ©. En quoi cela Ã©tait bloquant :\nLes tasks ECS ne pouvais pas accÃ©der aux diffÃ©rents secrets (login DB, tokens) sans assumer un rÃ´le ayant les bonnes policies de configurÃ©es La solution LiteStream a besoin d\u0026rsquo;un service account afin d\u0026rsquo;Ãªtre en mesure d\u0026rsquo;Ã©crire dans le bucket S3. L\u0026rsquo;ensemble des configurations que j\u0026rsquo;ai effectuÃ©es sont disponibles ici. Pour contourner cela, j\u0026rsquo;ai utilisÃ© mon compte Cloud Guru pour instancier des sandboxs non restreintes Ã  la volÃ©e. L\u0026rsquo;utilisation de OpenTofu a permis cela de faÃ§on aisÃ©e, Ã©tant donnÃ© que l\u0026rsquo;ensemble de ma configuration est Ã©crite. En outre, la sandbox ainsi crÃ©Ã©e m\u0026rsquo;as permis de crÃ©er une VM EC2 servant en tant que runner, nous permettant de continuer Ã  dÃ©ployer via les pipelines CI/CD.\nSi tu souhaites avoir plus de dÃ©tail sur l\u0026rsquo;ensemble du projet, je t\u0026rsquo;invite Ã  lire la documentation Ã©crite pour l\u0026rsquo;occasion.\nConclusion #Encore une vrai mise en pratique proposÃ©e par notre formateur Thomas Saquet. La conceptualisation, crÃ©ation, puis dÃ©ploiement d\u0026rsquo;un applicatif web de A Ã  Z est un bon exercice pour rÃ©ellement comprendre les enjeux auxquels sont confrontÃ©s un DevOps. En effet, il ne s\u0026rsquo;agit pas uniquement de construire une infrastructure, mais d\u0026rsquo;Ãªtre en mesure de proposer les solutions les plus pertinentes pour mettre en production un applicatif client.\n","date":"10 juin 2024","permalink":"/projets/projet_etude_master/","section":"Projets","summary":"Ce projet Ã  Ã©tÃ© rÃ©alisÃ© dans le cadre de ma premiÃ¨re annÃ©e de Master, en collaboration avec \u003ca href=\"https://gitlab.com/Toxma\" target=\"_blank\" rel=\"noreferrer\"\u003eMatteoz\u003c/a\u003e.","title":"IntÃ©gration et livraison continue : dÃ©ployer une API privÃ©e sur le cloud AWS"},{"content":" Cette page rassemble l\u0026rsquo;ensemble de mes projets scolaires et personnels rÃ©alisÃ©s en classe ou sur mon temps libre ","date":null,"permalink":"/projets/","section":"Projets","summary":"","title":"Projets"},{"content":" RÃ©alisÃ© dans le cadre de ma premiÃ¨re annÃ©e de Master, ce projet notÃ© Ã  pour but de concevoir une application en CLI permettant de gÃ©rer/configurer le backend IAM Casdoor.\nL\u0026rsquo;ensemble du code source est disponible en open-source sur GitLab\nIntroduction #Le but de ce projet, outre la dÃ©couverte de Golang est de rÃ©flÃ©chir Ã  la maniÃ¨re dont une application en CLI est construite. La seule contrainte imposÃ©e par le sujet Ã©tant le fait d\u0026rsquo;utiliser Go, et de choisir parmi 3 backends IAM Ã  savoir :\nCasdoor Authentik PocketBase Le choix des framework utilisÃ©s est complÃ¨tement libre. Pour ma part, j\u0026rsquo;ai choisi Cobra, de part sa maturitÃ© et sa massive adoption dans divers projets d\u0026rsquo;envergure (comme Docker CLI).\nJ\u0026rsquo;ai d\u0026rsquo;ailleurs dÃ©couvert que Hugo, le framework que j\u0026rsquo;utilise pour construire mon site web utilise Cobra pour sa CLI! ProblÃ©matique #GÃ©nÃ©ralitÃ©s #Avant de commencer Ã  coder, il faut savoir dans quelle direction on souhaite aller. La premiÃ¨re chose Ã  faire Ã©tant d\u0026rsquo;installer son environnement de dÃ©veloppement. Ici, ayant choisi Casdoor il Ã  fallu embarquer :\nUn container hÃ©bergeant l\u0026rsquo;application Casdoor Une base de donnÃ©e MySQL Un fichier permettant d\u0026rsquo;initialiser la base de donnÃ©e de Casdoor au boot Le but Ã©tant de permettre au correcteur de facilement tester mon application, l\u0026rsquo;ensemble se doit d\u0026rsquo;Ãªtre facilement vÃ©rifiable. Ici, un simple docker compose up -d permet de lancer l\u0026rsquo;ensemble. La documentation du projet se doit d\u0026rsquo;Ãªtre claire et agrÃ©able Ã  lire. Pour ce faire, j\u0026rsquo;ai rÃ©digÃ© un README.md dÃ©taillant l\u0026rsquo;ensemble des fonctionnalitÃ©s incluses ainsi qu\u0026rsquo;une documentation permettant d\u0026rsquo;utiliser mon programme.\nMon programme doit en outre Ãªtre en mesure de :\nCrÃ©er/Modifier/Supprimer (CRUD) les utilisateurs sur Casdoor IntÃ©grer une gestion des droits (qui peut faire quoi) Architecture du programme # Dans une interface en ligne de commande, chaque commande est exÃ©cutÃ©e sÃ©quentiellement, gÃ©nÃ©ralement de maniÃ¨re isolÃ©e. Contrairement Ã  une application graphique ou une API oÃ¹ l\u0026rsquo;Ã©tat peut Ãªtre maintenu en permanence en mÃ©moire vive, un programme CLI doit trouver des moyens alternatifs pour sauvegarder les Ã©tats entre les exÃ©cutions de commandes. Dans le cadre de mon projet, il s\u0026rsquo;agit d\u0026rsquo;Ãªtre en mesure de persister certaines donnÃ©es comme un token de login, de faÃ§on sÃ©curisÃ©e.\nLe framework Cobra utilisÃ© dans mon application propose une faÃ§on d\u0026rsquo;organiser son code en suivant une mÃ©thode bien prÃ©cise, dÃ©taillÃ©e dans sa documentation.\nRÃ©alisation #L\u0026rsquo;architecture de mon repo est prÃ©sentÃ©e comme ceci :\nâ”œâ”€â”€ LICENSE â”œâ”€â”€ Makefile â”œâ”€â”€ README.md â”œâ”€â”€ cmd â”‚Â â”œâ”€â”€ groups.go â”‚Â â”œâ”€â”€ login.go â”‚Â â”œâ”€â”€ oauth.go â”‚Â â”œâ”€â”€ root.go â”‚Â â””â”€â”€ users.go â”œâ”€â”€ conf.yml â”œâ”€â”€ config.yaml.example â”œâ”€â”€ dev â”‚Â â””â”€â”€ app.conf â”œâ”€â”€ docker-compose.yaml â”œâ”€â”€ go.mod â”œâ”€â”€ go.sum â”œâ”€â”€ handlers â”œâ”€â”€ helpers â”‚Â â”œâ”€â”€ authorize.go â”‚Â â”œâ”€â”€ roles.go â”‚Â â””â”€â”€ users.go â”œâ”€â”€ img â”‚Â â”œâ”€â”€ logo.png â”‚Â â”œâ”€â”€ screenshoot.png â”‚Â â”œâ”€â”€ screenshoot_1.png â”‚Â â”œâ”€â”€ screenshoot_2.png â”‚Â â”œâ”€â”€ screenshoot_3.png â”‚Â â””â”€â”€ t-rec.gif â”œâ”€â”€ img.png â”œâ”€â”€ init_data.json â”œâ”€â”€ logger â”‚Â â””â”€â”€ logger.go â”œâ”€â”€ main.go â”œâ”€â”€ models â”‚Â â””â”€â”€ models.go â””â”€â”€ utils â”œâ”€â”€ colors.go â”œâ”€â”€ keyring.go â””â”€â”€ table.go main.go #A la racine du dossier, on a diffÃ©rent fichiers dont le main.go permettant d\u0026rsquo;initialiser le programme. J\u0026rsquo;ai sÃ©parÃ© mon programme en diffÃ©rents dossier (modules Go) afin de garder une certaine cohÃ©rence lors du dÃ©veloppement.\ncmd #Ce dossier regroupe l\u0026rsquo;ensemble des commandes disponible dans ma CLI. Chaque fichier suit la mÃªme logique, et se base sur les mÃ©thodes dÃ©taillÃ©es dans la documentation de Cobra.\nhelpers #Les helpers regroupent des fonctions middlewares, ici permettant de gÃ©rer l\u0026rsquo;authentification OAuth2 et la gestion des rÃ´les utilisateurs.\nutils #Ce dossier regroupes quelques utilitaires que j\u0026rsquo;ai dÃ©veloppÃ©, comme la gestion des couleurs dans l\u0026rsquo;output, et le backend permettant de sauvegarder des tokens de faÃ§on sÃ©curisÃ© (go-keyring).\nConclusion #Pour une revue plus en dÃ©tail du projet, je t\u0026rsquo;invites Ã  consulter le repo disponible ici. Le README.md dÃ©taille l\u0026rsquo;ensemble des procÃ©dures nÃ©cessaires pour lancer l\u0026rsquo;outil.\nCe projet fÃ»t ma rÃ©elle premiÃ¨re interaction avec Golang. J\u0026rsquo;y ai dÃ©couvert un langage trÃ¨s typÃ©, assez simple Ã  apprÃ©hender. Je pense en approfondir la maÃ®trise, car la plupart des outils DevOps les plus courants sont codÃ©s en Go.\n","date":"6 avril 2024","permalink":"/projets/golang_cli/","section":"Projets","summary":"RÃ©alisÃ© dans le cadre de ma premiÃ¨re annÃ©e de Master, ce projet notÃ© Ã  pour but de concevoir une application en CLI permettant de gÃ©rer/configurer le backend IAM \u003ca href=\"https://casdoor.org/\" target=\"_blank\" rel=\"noreferrer\"\u003eCasdoor\u003c/a\u003e.","title":"Golang \u0026 CobraCLI : rÃ©alisation d'une application en ligne de commande"},{"content":" RÃ©alisÃ© dans le cadre de mon annÃ©e de Bachelor, le Hackaton Ã  pour but de proposer une solution informatique en une semaine. Un jury juge ensuite la solution la plus convaincante et dÃ©signe les vainqueurs.\nL\u0026rsquo;ensemble du projet que j\u0026rsquo;ai rÃ©alisÃ© est disponible en open-source\nsur GitLab\nEtant encore Ã©tudiant et loin d\u0026rsquo;Ãªtre un spÃ©cialiste K8S, le projet prÃ©sentÃ© sur cette page comporte forcÃ©ment de grosses imprÃ©cisions et d\u0026rsquo;Ã©normes failles de sÃ©curitÃ©. Si par hasard un DevOps confirmÃ© tombe sur ces lignes, n\u0026rsquo;hÃ©sites pas Ã  me contacter via la section commentaire ou via email, j\u0026rsquo;aurais quelques questions Ã  te poser ğŸ˜‰ Introduction #Lors de ce hackathon, mon Ã©quipe Ã  choisi pour sujet Solution Libre. Il s\u0026rsquo;agissait de :\ndÃ©velopper le frontend d\u0026rsquo;une application Ã  destination de formateurs et de leurs Ã©lÃ¨ves dÃ©velopper l\u0026rsquo;architecture qui permettra in fine d\u0026rsquo;hÃ©berger l\u0026rsquo;application, son backend et sa base de donnÃ©es proposer une CI/CD permettant de mettre en place une livraison et une intÃ©gration continue de l\u0026rsquo;applicatif et son infrastructure Le back-end Ã©tant fourni par l\u0026rsquo;Ã©cole, mon travail ici se limitait Ã  son intÃ©gration ainsi qu\u0026rsquo;a celle du front-end.\nContexte et travail d\u0026rsquo;Ã©quipe #L\u0026rsquo;ensemble des Ã©tudiants de l\u0026rsquo;Ã©cole sont rÃ©unis en Ã©quipe de 10, tout niveaux et spÃ©cialitÃ©s confondues. Toute la difficultÃ© Ã©tant de coordonner l\u0026rsquo;ensemble de l\u0026rsquo;Ã©quipe afin de livrer un produit fonctionnel.\nLimites et difficultÃ©s #Malheureusement, l\u0026rsquo;Ã©quipe dont j\u0026rsquo;ai fais partie n\u0026rsquo;as pas sÃ» dÃ©livrer un front-end dans les temps impartis. Cela dit, de mon cÃ´tÃ© j\u0026rsquo;ai pu architecturer l\u0026rsquo;ensemble de l\u0026rsquo;infrastructure ainsi que la pipeline CI/CD et obtenir un POC fonctionnel. Il ne me manquais plus qu\u0026rsquo;un front-end afin d\u0026rsquo;obtenir le rÃ©sultat demandÃ©, dommage ğŸ˜¥!\nCependant, j\u0026rsquo;ai pu tester le fonctionnement du back-end sur l\u0026rsquo;architecture ainsi dÃ©ployÃ©e. C\u0026rsquo;est d\u0026rsquo;ailleurs l\u0026rsquo;objet de cet article : prÃ©senter ma solution sur la partie DevOps ğŸ˜.\nSchÃ©ma d\u0026rsquo;architecture #J\u0026rsquo;ai rÃ©alisÃ© un petit brouillon de l\u0026rsquo;architecture que j\u0026rsquo;ai souhaitÃ© mettre en place sur le cloud Azure :\nOn Ã  donc:\nUn cluster Kubernetes managÃ© sur le cloud Azure (AKS) 3 namepsaces : un pour le front, un pour le back et le dernier pour le monitoring et la gestion des logs Lors de la rÃ©alisation, j\u0026rsquo;ai regroupÃ© tout mes pods K8S au sein du mÃªme namespace par manque de temps lors du dÃ©veloppement de la partie Terraform. La partie monitoring est aussi inachevÃ©e et ne figure pas sur le repo. Cela dit, il n\u0026rsquo;est pas exclu que je m\u0026rsquo;y penche dans le cadre d\u0026rsquo;un article de blog dans un futur proche. En l\u0026rsquo;Ã©tat, l\u0026rsquo;architecture disponible sur le lien GitLab donnÃ©e en prÃ©ambule permet uniquement de requÃªter l\u0026rsquo;API du backend. De plus, comme indiquÃ© sur le schÃ©ma la base de donnÃ©e utilisÃ©e est hÃ©bergÃ©e dans un pod. J\u0026rsquo;aurais prÃ©fÃ©rÃ© mettre en place une base de donnÃ©e managÃ©e mais n\u0026rsquo;Ã©tant pas encore trÃ¨s Ã  l\u0026rsquo;aise avec Kubernetes sur Azure, j\u0026rsquo;ai prÃ©fÃ©rÃ© aller au plus simple. Le but de cet article reste pour moi l\u0026rsquo;occasion de garder une sorte de documentation sur le projet rÃ©alisÃ©. A ne pas utiliser en production donc ğŸ˜‰.\nInfrastructure As Code #Contexte #L\u0026rsquo;enjeu Ã©tant de provisioner tout cela as code, j\u0026rsquo;ai dÃ©ployÃ© cette architecture avec Terraform. Le repo contenant l\u0026rsquo;infrastructure est construit comme ceci:\nâ””â”€â”€ terraform â”œâ”€â”€ aks.tf â”œâ”€â”€ environment â”‚Â â””â”€â”€ dev â”‚Â â””â”€â”€ variables.tfvars â”œâ”€â”€ kubernetes.tf â”œâ”€â”€ main.tf â”œâ”€â”€ modules â”‚Â â””â”€â”€ kubernetes â”‚Â â”œâ”€â”€ main.tf â”‚Â â”œâ”€â”€ outputs.tf â”‚Â â””â”€â”€ variables.tf â”œâ”€â”€ outputs.tf â”œâ”€â”€ rg.tf â”œâ”€â”€ variables.tf â””â”€â”€ vpc.tf Cette architecture est dÃ¨s le dÃ©part conÃ§ue pour Ãªtre en mesure de dÃ©ployer des environments ISO dev/pprd/prod. Cela permet au dÃ©veloppeur d\u0026rsquo;Ãªtre en mesure (en thÃ©orie) de tester son code en amont sur des architectures identiques avant de dÃ©ployer l\u0026rsquo;applicatif en production.\nBackend et providers #Dans le fichier main.tf, en dÃ©but de code on retrouve la dÃ©claration du backend, ainsi que les providers requis:\nterraform { backend \u0026#34;azurerm\u0026#34; { resource_group_name = \u0026#34;backend-terraform-rg\u0026#34; storage_account_name = \u0026#34;backendhackatonsdv\u0026#34; container_name = \u0026#34;terraform-state\u0026#34; key = \u0026#34;terraform.tfstate\u0026#34; } required_providers { azurerm = { source = \u0026#34;hashicorp/azurerm\u0026#34; version = \u0026#34;3.63.0\u0026#34; } kubernetes = { source = \u0026#34;hashicorp/kubernetes\u0026#34; version = \u0026#34;2.21.1\u0026#34; } } } provider \u0026#34;azurerm\u0026#34; { features {} skip_provider_registration = true } provider \u0026#34;kubernetes\u0026#34; { host = module.aks.host client_certificate = base64decode(module.aks.client_certificate) client_key = base64decode(module.aks.client_key) cluster_ca_certificate = base64decode(module.aks.cluster_ca_certificate) } Le backend, permettant de stocker le fichier tfstate consiste en un compte de stockage Azure hÃ©bergeant ce mÃªme fichier. Dans le cas de ce lab, j\u0026rsquo;ai utilisÃ© un compte de stockage prÃ©alablement existant sur mon compte Azure.\nJ\u0026rsquo;utilise les providers azure_rm et kubernetes permettant de respectivement :\nintÃ©ragir avec mon compte Azure intÃ©ragir avec le cluster Kubernetes dÃ©ployÃ© J\u0026rsquo;utilise un compte Azure Students fourni par l\u0026rsquo;Ã©cole pour ce lab. Terraform refusais systÃ©matiquement d\u0026rsquo;apply mon infrastructure en raison d\u0026rsquo;erreurs de droits. L\u0026rsquo;option skip_provider_registration = true m\u0026rsquo;as permis de dÃ©bloquer la situation. Le provider kubernetes nÃ©cessite une configuration permettant de se connecter au cluster afin d\u0026rsquo;y dÃ©ployer les ressources. Ici, je fournis les valeurs dynamiquement Ã  partir du module aks que je decris plus loin dans l\u0026rsquo;article.\nTerraform et modules #Pour une meilleure lisibilitÃ©, je prÃ©fÃ¨re sÃ©parer chaque ressources dÃ©ployÃ©es en un fichier distinct. Cela Ã©vite de se retrouver avec un gros fichier de plusieurs centaines de lignes devenant vite illisible. Terraform permet ensuite d\u0026rsquo;organiser son code en modules. Un module est un ensemble de fichiers Terraform stockÃ©s dans un dossier. Cela permet d\u0026rsquo;Ã©viter les rÃ©pÃ©titions dans le code et la mÃ©thode de fonctionnement peut Ãªtre comparable Ã  une fonction dans un programme informatique. Ici j\u0026rsquo;ai donc :\naks.tf -\u0026gt; CrÃ©e un cluster Kubernetes managÃ© via le module Azure/aks/azurerm kubernetes.tf -\u0026gt; IntÃ©ragit avec le cluster Kubernetes. Ce fichier appelle un module que j\u0026rsquo;ai dÃ©veloppÃ© et stockÃ© dans le rÃ©pertoire modules rg.tf -\u0026gt; Le groupe de ressource Azure contenant l\u0026rsquo;ensemble des instances vpc.tf -\u0026gt; La configuration rÃ©seau de l\u0026rsquo;infrastructure Pour finir, dans le fichier variables.tf, je dÃ©clare les variables qui seront utilisÃ©es pour dÃ©ployer l\u0026rsquo;infrastructure. Ces variables sont fournies par le fichier variables.tfvars, qui diffÃ¨re en fonction de l\u0026rsquo;environement de production choisi. Ainsi, au plan ou apply il suffira de rajouter le flag -var-file=environment/dev/variables.tfvars afin de dÃ©finir l\u0026rsquo;environment choisi. Dans ce lab, il s\u0026rsquo;agit de l\u0026rsquo;environment de dev.\nCI/CD #Containerisation #Pour Ãªtre en mesure de dÃ©ployer l\u0026rsquo;infrastructure sur Kubernetes, il m\u0026rsquo;a fallu au prÃ©alable containairiser le back-end dans une image Docker prÃªte Ã  Ãªtre stockÃ© sur un registre d\u0026rsquo;images (ici, j\u0026rsquo;utilise celui de GitLab). La mÃ©thode aurais Ã©tÃ© similaire pour le front-end. J\u0026rsquo;ai donc Ã©cris un Dockerfile :\n# Base Golang Image FROM golang:latest # Setup working directory WORKDIR /usr/src/osf-core # Copy source code to COPY . /usr/src/osf-core # Install Git and NodeJS RUN curl -sL https://deb.nodesource.com/setup_16.x | bash - RUN apt-get install -y nodejs npm # Install NPM dependencies RUN npm install -g @marp-team/marp-core \\ \u0026amp;\u0026amp; npm install -g markdown-it-include \\ \u0026amp;\u0026amp; npm install -g markdown-it-container \\ \u0026amp;\u0026amp; npm install -g markdown-it-attrs # Install Go Library \u0026amp; Swagger RUN cd /usr/src/osf-core \u0026amp;\u0026amp; go get golang.org/x/text/transform \\ \u0026amp;\u0026amp; go get golang.org/x/text/unicode/norm \\ \u0026amp;\u0026amp; go install github.com/swaggo/swag/cmd/swag@v1.8.12 # Init Swagger RUN cd /usr/src/osf-core \u0026amp;\u0026amp; swag init --parseDependency --parseInternal # Export ports EXPOSE 8000/tcp EXPOSE 443/tcp EXPOSE 80/tcp # Launch the API CMD [\u0026#34;go\u0026#34;, \u0026#34;run\u0026#34;, \u0026#34;/usr/src/osf-core/main.go\u0026#34;] J\u0026rsquo;ai tentÃ© d\u0026rsquo;utiliser les fichiers packages.json et go.sum/go.mod afin d\u0026rsquo;installer les dÃ©pendances directement depuis ces fichiers, mais la gÃ©nÃ©ration de mon image plantait. De plus, j\u0026rsquo;ai du forcer la version de swagger en 1.8.12 car un problÃ¨me de compatibilitÃ© m\u0026rsquo;empÃªchais de gÃ©nÃ©rer la documentation Swagger.\nCe Dockerfile est utilisÃ© dans la CI afin de gÃ©nÃ©rer dynamiquement l\u0026rsquo;image destinÃ© Ã  Ãªtre poussÃ©e dans le cluster K8S.\nCI applicative #La CI applicative est trÃ¨s basique pour ce lab mais il y a quelques spÃ©cificitÃ©s. J\u0026rsquo;utilise une image docker in docker, car pour builder l\u0026rsquo;image applicative il est nÃ©cessaire d\u0026rsquo;executer des commandes docker dans docker (plus d\u0026rsquo;infos ici). Pour fonctionner correctement, le build d\u0026rsquo;une image Docker par le runner nÃ©cessite d\u0026rsquo;ajouter ces lignes en dÃ©but de CI :\nimage: docker:20.10.16 services: - docker:20.10.16-dind variables: DOCKER_TLS_CERTDIR: \u0026#34;/certs\u0026#34; Sans cela, docker serais incapable d\u0026rsquo;accÃ©der Ã  Internet Ã  l\u0026rsquo;intÃ©rieur du container.\nLa CI consiste en 3 stages :\ntest deploy trigger_deploy_to_terraform 2 variables sont Ã  renseigner manuellement : ENV et TAG:\nENV: value: \u0026#34;dev\u0026#34; description: \u0026#34;On wich env the image should be deployed\u0026#34; TAG: value: \u0026#34;latest\u0026#34; description: \u0026#34;Version of the image\u0026#34; Ces variables sont utilisÃ©es par la suite pour dÃ©finir sur quel environment dÃ©ployer l\u0026rsquo;image, et permet Ã  Terraform de rÃ©cupÃ©rer le chemin vers l\u0026rsquo;image Ã  pousser sur le cluster. Le script qui va crÃ©er l\u0026rsquo;image est trÃ¨s simple :\ndeploy: stage: deploy script: - docker login -u $CI_REGISTRY_USER -p $CI_REGISTRY_TOKEN $DOCKER_REGISTRY_URL - docker build -t registry.gitlab.com/sdv-open-course-factory/ocf-core/${ENV}-backend:${TAG} . - docker push registry.gitlab.com/sdv-open-course-factory/ocf-core/${ENV}-backend:${TAG} Le nom et le chemin de l\u0026rsquo;image sera renseignÃ© automatiquement en fonction des donnÃ©es entrÃ©es par le dev.\nPour finir, le dernier stage dÃ©clenche la CI situÃ©es sur le repo Terraform, en poussant la variable TAG permettant Ã  Terraform de pousser la bonne image du backend sur le cluster Kubernetes:\nNew job to trigger the other project\u0026#39;s CI trigger_deploy_to_terraform: image: curlimages/curl stage: trigger_deploy_to_terraform script: - curl -X POST --fail -F token=$CI_TRIGGER_TOKEN -F \u0026#34;ref=main\u0026#34; -F \u0026#34;variables[TAG]=$TAG\u0026#34; https://gitlab.com/api/v4/projects/47370418/trigger/pipeline needs: - deploy Je n\u0026rsquo;ai pas encore variabilisÃ© l\u0026rsquo;environement Ã  ce niveau. Pour le moment, ma CI est uniquement en mesure de dÃ©ployer sur dev. Encore une fois, question de prioritÃ©s niveau timing. Je cherchais surtout Ã  avoir quelque chose de fonctionnel au plus vite. Ainsi, seul la variable TAG est envoyÃ©e Ã  Terraform, permettant de retrouver l\u0026rsquo;image. CI infrastructure #La CI d\u0026rsquo;infrastructure peut soit Ãªtre dÃ©clenchÃ©e par le job trigger de la CI applicative, soit manuellement. On retrouve les stages classiques :\nvalidate plan apply destroy J\u0026rsquo;utilise ici l\u0026rsquo;image hashicorp/terraform:latest, m\u0026rsquo;Ã©vitant d\u0026rsquo;installer Terraform Ã  chaque dÃ©ploiement sur le runner.\nAu niveau des variables :\nvariables: TF_ROOT: ${CI_PROJECT_DIR}/terraform/ TF_ENVIRONMENT: \u0026#34;dev\u0026#34; # DÃ©finissez l\u0026#39;environnement souhaitÃ© ici (par exemple, dev, preprod, prod) TF_DESTROY: description: Destroy Terraform resources value: \u0026#34;false\u0026#34; Comme dit prÃ©cÃ©demment, l\u0026rsquo;environement et codÃ© en dur sur dev dans mon lab La variable TF_DESTROY me permet de dÃ©truire l\u0026rsquo;infrastructure via la CI. Le stage destroy ne s\u0026rsquo;execute seulement ci la valeur est changÃ©e pour true:\nterraform_destroy: stage: destroy script: terraform destroy -auto-approve -var-file=environment/${TF_ENVIRONMENT}/variables.tfvars -var=\u0026#34;img_tag=${TAG}\u0026#34; rules: - if: $TF_DESTROY == \u0026#34;true\u0026#34; when: always La partie plan contient un simple script bash qui va tester l\u0026rsquo;existence de la variable TAG rÃ©cupÃ©rÃ©e depuis la CI applicative :\nif [ -n \u0026#34;$TAG\u0026#34; ]; then terraform plan -var-file=environment/${TF_ENVIRONMENT}/variables.tfvars -var \u0026#34;img_tag=${TAG}\u0026#34; else echo \u0026#34;Nothing to plan\u0026#34; fi J\u0026rsquo;utilise le flag -var-file=environment/${TF_ENVIRONMENT}/variables.tfvars afin de dÃ©terminer dynamiquement sur quel environenment dÃ©ployer. Puis le flag -var \u0026quot;img_tag=${TAG} pour dÃ©terminer l\u0026rsquo;image Docker Ã  utiliser sur le mÃªme principe.\nLe flag -var \u0026quot;img_tag=${TAG} permet de dÃ©finir la variable Terraform contenue dans le variables.tf Ã  la racine ainsi que dans le module Kubernetes:\nvariable \u0026#34;img_tag\u0026#34; { description = \u0026#34;Image tag\u0026#34; type = string } L\u0026rsquo;adresse permettant de pointer vers l\u0026rsquo;image est ensuite reconstruite comme ceci lors de la crÃ©ation du dÃ©ploiement Kubernetes:\nspec { container { image = \u0026#34;registry.gitlab.com/sdv-open-course-factory/ocf-core/${var.env}-backend:${var.img_tag}\u0026#34; name = \u0026#34;ocf-core-backend\u0026#34; port { container_port = 80 } port { container_port = 443 } port { container_port = 8000 } Conclusion #J\u0026rsquo;ai eu au final Ã  peu prÃ¨s 4 jours pour rÃ©aliser toute cette architecture. C\u0026rsquo;est assez court, mais le POC est fonctionnel et permet d\u0026rsquo;accÃ©der Ã  la doc swagger du backend et de manipuler l\u0026rsquo;API. Dans l\u0026rsquo;idÃ©e, pour vraiment dÃ©ployer ce lab en production il faudrais:\nBien sÃ»r y intÃ©grer un front-end Retravailler la CI pour intÃ©grer l\u0026rsquo;image du front-end IntÃ©grer toute la partie monitoring et gestion des logs, c\u0026rsquo;est super important Utiliser une BDD managÃ©e Azure au lieu d\u0026rsquo;une image PostegrSQL dans le cluster SÃ©parer chaque services sur des namespaces diffÃ©rents Niveau sÃ©curitÃ©, placer le loadbalancer sur un subnet sÃ©parÃ©. Mettre en place un bastion sur un VPC diffÃ©rent afin d\u0026rsquo;Ãªtre en mesure de requÃªter l\u0026rsquo;API K8S via kubectl. Ici, âš ï¸ l\u0026rsquo;API est exposÃ©e au web âš ï¸ Je pense rÃ©-utiliser mon architecture pour tenter de rÃ©aliser le front-end. J\u0026rsquo;en profiterais pour consolider le tout par rapport Ã  la liste ci-dessus. Cela fais aussi quelque temps que je suis intÃ©ressÃ© par les technologies de dev orientÃ© front-end (React) et n\u0026rsquo;ayant pas ou trÃ¨s peu d\u0026rsquo;expÃ©rience en JavaScript, ce serais l\u0026rsquo;occasion. Affaire Ã  suivre donc\u0026hellip;\n","date":"3 juillet 2023","permalink":"/projets/hackaton_sdv/","section":"Projets","summary":"RÃ©alisÃ© dans le cadre de mon annÃ©e de Bachelor, le Hackaton Ã  pour but de proposer une solution informatique en une semaine. Un jury juge ensuite la solution la plus convaincante et dÃ©signe les vainqueurs.","title":"Hackaton Sup De Vinci 2023"},{"content":"","date":null,"permalink":"/categories/aws/","section":"Categories","summary":"","title":"Aws"},{"content":"","date":null,"permalink":"/tags/awscli/","section":"Tags","summary":"","title":"Awscli"},{"content":"","date":null,"permalink":"/tags/cloudformation/","section":"Tags","summary":"","title":"Cloudformation"},{"content":"","date":null,"permalink":"/tags/ec2/","section":"Tags","summary":"","title":"Ec2"},{"content":"","date":null,"permalink":"/tags/gaming/","section":"Tags","summary":"","title":"Gaming"},{"content":"Introduction #Le cloud-gaming se dÃ©mocratise de plus en plus notamment par le biais d\u0026rsquo;acteurs tel que OVH avec son offre Shadow, ou encore NVDIA avec GeForce Now. Je me suis toujours demandÃ© par quel procÃ©dÃ© il serais possible de crÃ©er mon propre serveur cloud dÃ©diÃ© au jeu, et surtout si c\u0026rsquo;Ã©tait viable. AprÃ¨s quelques recherches, j\u0026rsquo;ai dÃ©couvert quelques pistes assez intÃ©ressantes, notament du cÃ´tÃ© d\u0026rsquo;AWS qui propose des instances de calcul avec GPU plutÃ´t abordables : les instances G4.\nAttention Ã  la facture si tu oublies d\u0026rsquo;Ã©teindre ou de supprimer ton instance aprÃ¨s avoir dÃ©roulÃ© l\u0026rsquo;article! Cet article te permettra de monter ta propre VM GPU, tout en te faisant dÃ©couvrir aws cli et CloudFormation, l\u0026rsquo;outil d\u0026rsquo;IAC proposÃ© par AWS.\nLes types d\u0026rsquo;instances G4 #AWS propose deux architectures, l\u0026rsquo;une est basÃ©e sur les puces GPU NVIDIA T4 (g4dn), l\u0026rsquo;autre sur des puces AMD Radeon Pro V520 (g4ad). Ci-dessous un tableau comparatif du rapport prix/performances des diffÃ©rentes instances :\nInstance Type 3DMark Score On-demand Price (us-east-1, USD, 02/23) Price-performance (3DMark points / $) g4dn.xlarge 4300 $0.71 6056 g4dn.2xlarge 4800 $1.12 4286 g4dn.4xlarge 6000 $1.94 3093 g4ad.xlarge 5100 $0.56 9107 g4ad.2xlarge 6600 $0.91 7253 g4ad.4xlarge 7600 $1.60 4750 g5.xlarge 6800 $1.19 5714 g5.2xlarge 10200 $1.58 6456 g5.4xlarge 13000 $2.36 5508 Source : https://github.com/aws-samples/cloud-gaming-on-ec2-instances\nPour cet article, je me base sur une instance AMD g4ad.xlarge, largement suffisante pour mes besoins.\nPrÃ©requis # Un compte AWS awscli d\u0026rsquo;installÃ© et de configurÃ© (voir la doc AWS) Assez de quota sur ton compte AWS pour provisionner des instances GPU G4 (j\u0026rsquo;explique plus bas comment en obtenir) A noter: Il n\u0026rsquo;y a pas de free tier proposÃ© par AWS sur les instances GPU. Il est essentiel de bien Ã©teindre ou dÃ©truire ta VM une fois la session terminÃ©e afin Ã©viter d\u0026rsquo;Ãªtre facturÃ© pour des heures inutilisÃ©es\u0026hellip; Infrastructure cible #Mon template CloudFormation permet de gÃ©nÃ©rer :\nUn VPC dÃ©diÃ© Un subnet public Une instance EC2 g4adn.xlarge avec le systÃ¨me sur un disque SSD, avec un volume HDD (st1) montÃ© pour stocker les jeux Je me suis basÃ© sur l\u0026rsquo;ami Microsoft Windows Server 2019 with AMD Radeon Pro Driver proposÃ©e gratuitement par AWS pour gÃ©nÃ©rer mon template. Elle est prÃ©configurÃ©e avec les drivers AMD prÃ©installÃ©s (pratique).\nLes disques st1 sont relativement lents, mais peu cher. J\u0026rsquo;ai choisi cette option ici afin de tester ma config, mais il est conseillÃ© d\u0026rsquo;utiliser un stockage SSD pour jouer confortablement. DÃ©ploiement #Quotas de services #Les instances G4 AWS nÃ©cessitent des quotas pour Ãªtre provisionnÃ©es. Par dÃ©faut, sur ce type d\u0026rsquo;instance, le quota est de 0. Si tu essayes de provisionner une machine G4 en ayant 0 quota, Ã§a ne marchera pas. Pour ce faire, connecte toi Ã  la console AWS et tape service quotas dans la barre de recherche et choisi EC2 :\nDans la barre de recherche sur l\u0026rsquo;Ã©cran suivant, tape \u0026ldquo;G4\u0026rdquo; et suit les instructions pour augmenter le quota. Le quota correspond au nombres de vCPU allouÃ©es Ã  une instance. Pour la g4ad.xlarge, il nous faut 4 vCPU. Changer la valeur du quota par 4, et envoyer.\nIl n\u0026rsquo;est pas possible d\u0026rsquo;obtenir des quotas pour des host dedicated avec un compte AWS personnel qui a peu servi. AprÃ¨s avoir fait la demande, le service client d\u0026rsquo;AWS la refusera, et te proposera des quotas pour des instances de type spot ou on-demand. J\u0026rsquo;ai demandÃ© d\u0026rsquo;avoir accÃ¨s aux instances on-demand suite Ã  ma premiÃ¨re requÃªte, celles-ci furent disponible le lendemain. DÃ©ploiement de l\u0026rsquo;EC2 #Si ce n\u0026rsquo;est pas dÃ©jÃ  fais, authentifie toi Ã  awscli. Si tu as besoin d\u0026rsquo;aide, la doc d\u0026rsquo;Amazon est vraiment bien faite.\nÃ‰tant sous MacOS, les commandes suivantes devraient fonctionner sous Linux et MacOS. Si tu es sous Windows, je te conseille de passer via WSL pour bÃ©nÃ©ficier d\u0026rsquo;un shell bash. Mon template utilise une KeyPair appelÃ©e CloudGamingKeyPair permettant de decrypter le mot de passe administrateur de l\u0026rsquo;instance. Pour la crÃ©er :\nmkdir ~/.ssh/aws-private-keys \u0026amp;\u0026amp; cd ~/.ssh/aws-private-keys aws ec2 create-key-pair \\ --key-name CloudGamingKeyPair \\ --query \u0026#39;KeyMaterial\u0026#39; \\ --region eu-west-2 \\ --output text \u0026gt; CloudGamingKeyPair.pem Clone mon repo et ce rendre dans le dossier contenant le template :\ngit clone https://github.com/fabienchevalier/ec2-cloudgaming \u0026amp;\u0026amp; cd cloudformation Ouvre le template avec un editeur de texte, et modifie les lignes 60 et 64 en remplacant l\u0026rsquo;adresse par ton adresse IP.\nSecurityGroupIngress: - IpProtocol: tcp FromPort: 8443 #NICE DVC Server ToPort: 8443 CidrIp: 0.0.0.0/0 #Replace that with your IP address (mask should be /32) - IpProtocol: tcp FromPort: 3389 ToPort: 3389 CidrIp: 0.0.0.0/0 #Repl 0.0.0.0/0 autorise n\u0026rsquo;importe quelle adresse sur le port RDP et NICE DCV Server, ce qui n\u0026rsquo;est pas souhaitable.\nTip: tu peux rÃ©cupÃ©rer ton adresse ip public via curl ifconfig.me DÃ©ploie le template CloudFormation :\naws --region eu-west-2 cloudformation deploy \\ --template deploy-cloud-gaming-ec2.cfn.yaml \\ --stack-name CloudGamingStack Il est possible de suivre l\u0026rsquo;avancement de la crÃ©ation de la stack sur la console AWS, via la page CloudFormation : Une fois l\u0026rsquo;instance dÃ©ployÃ©e, il faut rÃ©cupÃ©rer le mot de passe Administrateur permettant une premiÃ¨re connexion via le protocole RDP.\nOn rÃ©cupÃ¨re l\u0026rsquo;ID de l\u0026rsquo;instance crÃ©e :\naws --region eu-west-2 ec2 describe-instances \\ --filters \u0026#34;Name=tag:Name,Values=CloudGamingInstance\u0026#34; \\ --query \u0026#39;Reservations[].Instances[].[InstanceId]\u0026#39; \\ --output text Copier l\u0026rsquo;ID gÃ©nÃ©rÃ© puis :\naws ec2 get-password-data --instance-id i-1234567890abcdef0 \\ --priv-launch-key ~/.ssh/aws-private-keys/CloudGamingKeyPair.pem Le mot de passe Administrateur se situe dans le champ PasswordData de l\u0026rsquo;output, par exemple :\n{ \u0026#34;InstanceId\u0026#34;: \u0026#34;i-1234567890abcdef0\u0026#34;, \u0026#34;Timestamp\u0026#34;: \u0026#34;2013-08-30T23:18:05.000Z\u0026#34;,z \u0026#34;PasswordData\u0026#34;: \u0026#34;\u0026amp;ViJ652e*u\u0026#34; } On y est presque!\nConfiguration de l\u0026rsquo;instance #Disque dur #Une premiÃ¨re connexion via RDP est nÃ©cessaire afin de configurer quelques derniers dÃ©tails. Pour ce faire, lance un client RDP (Microsoft Remote Desktop sous MacOS pour ma part) et connecte toi Ã  l\u0026rsquo;instance via son IP publique.\nUtilise les informations de login rÃ©cupÃ©rÃ©es auparavant (login Administrator, mot de passe donnÃ© par la commande ec2 get-password-data) pour te connecter. Cherches disk manager dans la barre de recherche Windows, et formate le disque crÃ©Ã© par le template CloudFormation comme ceci :\nChoisir l\u0026rsquo;option New Simple Volume, et formater l\u0026rsquo;ensemble de l\u0026rsquo;espace disque disponible.\nServeur Nice DCV #Il nous reste Ã  dÃ©ployer le serveur Nice DCV permettant de streamer sans latences depuis l\u0026rsquo;instance EC2. Pour ce faire, il faudra se connecter une premiÃ¨re fois en RDP via l\u0026rsquo;adresse IP publique de l\u0026rsquo;EC2 afin d\u0026rsquo;executer ce script PowerShell :\n# Set TLS 1.2 for Invoke-RestMethod [Net.ServicePointManager]::SecurityProtocol = [Net.SecurityProtocolType]::Tls12 # Set the download link to Nice DCV Server (64-bit installer) $downloadUrl = \u0026#34;https://d1uj6qtbmh3dt5.cloudfront.net/nice-dcv-server-x64-Release.msi\u0026#34; # Set the path for our download, which will be in the temp directory $installerFile = \u0026#34;nice-dcv-server-x64-Release.msi\u0026#34; $installerDownloadPath = (Join-Path $env:TEMP $installerFile) # Set the default owner to the current user $installerOwner = [Environment]::UserName # Set Install Command Expression $msiExpression = \u0026#34;msiexec.exe /i $installerDownloadPath AUTOMATIC_SESSION_OWNER=$installerOwner ADDLOCAL=ALL /quiet /norestart /l*v dcv_install_msi.log\u0026#34; # Download the file Invoke-Webrequest $downloadUrl -UseBasicParsing -OutFile $installerDownloadPath # Install Invoke-Expression $msiExpression Pour ce faire, copie/colle ce script dans le notepad de l\u0026rsquo;instance, et enregistre le fichier sur le bureau :\nPuis, clic droit sur l\u0026rsquo;icÃ´ne, puis Run with PowerShell. Attendre une ou deux minutes aprÃ¨s que la fenÃªtre se ferme, le temps que le serveur se lance.\nThat\u0026rsquo;s it! Une fois le serveur dÃ©ployÃ©, tu peux fermer ta connexion RDP, et tÃ©lÃ©charger le client Nice DCV.\nConnexion Ã  l\u0026rsquo;instance via Nice DCV #Lance le client, et connecte toi via l\u0026rsquo;adresse IP publique de ton instance, sur le port 8443:\nUne erreur de certificat peut apparaÃ®tre, clique sur Trust \u0026amp; Connect\nA partir de lÃ , libre Ã  toi d\u0026rsquo;installer tes jeux et de commencer Ã  jouer !\nJe te conseilles de dÃ©sactiver le mode de sÃ©curitÃ© d\u0026rsquo;IE, navigateur par dÃ©faut sur Windows Server (hÃ© oui\u0026hellip;) puis d\u0026rsquo;installer un navigateur rÃ©cent. Quelques amÃ©liorations #Par dÃ©faut, le serveur NICE DCV streame 25 FPS. C\u0026rsquo;est configurable via le registre Windows, via la clÃ©e situÃ©e ici : HKEY_CURRENT_USER\\Software\\GSettings\\com\\nicesoftware\\dcv. CrÃ©er une nouvelle clÃ©e display et lui donner pour valeur (type DWORD 32bits) le nombre de FPS souhaitÃ©.\nConclusion #AprÃ¨s quelques heures d\u0026rsquo;essais, le confort de jeu est plutÃ´t bon. Pas trop de latences, mÃªme en WiFi. La qualitÃ© d\u0026rsquo;image est aussi plutÃ´t bonne.\nCependant, le coÃ»t engendrÃ© par l\u0026rsquo;utilisation de l\u0026rsquo;EC2 reste trop Ã©levÃ© Ã  mon sens pour que cela soit viable. En effet, il faut ajouter au prix horaire de l\u0026rsquo;instance :\nLe stockage (assez cher si on choisit de passer par du SSD : +/- 11$/mois) La bande passante : payÃ©e au Go, Ã§a peut vite s\u0026rsquo;envoler pour du stream 4K@60FPS. Cela dit, cette mÃ©thode de tarification Ã  l\u0026rsquo;heure peut convenir Ã  des joueurs occasionels ne voulant pas s\u0026rsquo;abonner Ã  un service qu\u0026rsquo;ils utiliserons que trÃ¨s peu.\nIl est aussi possible d\u0026rsquo;utiliser Parsec pour streamer le contenu du serveur, mais dans sa version gratuite la rÃ©solution maximale est de 1080p. Si tu remarques des fautes ou quelque chose qui ne fonctionne pas, n\u0026rsquo;hÃ©sites pas Ã  le mentionner dans les commentaires !\nA bientÃ´t !\n","date":"30 mars 2023","permalink":"/blog/informatique/aws-ec2-cloudgaming-instance/","section":"Blog","summary":"Provisionner facilement son instance gaming en utilisant CloudFormation et NICE DCV Server sur le cloud AWS.","title":"Provisionner sa propre instance de jeu Windows Server dans le cloud AWS via CloudFormation"},{"content":"","date":null,"permalink":"/tags/windows-server-2019/","section":"Tags","summary":"","title":"Windows-Server-2019"},{"content":"","date":null,"permalink":"/tags/appservices/","section":"Tags","summary":"","title":"Appservices"},{"content":"","date":null,"permalink":"/categories/azure/","section":"Categories","summary":"","title":"Azure"},{"content":"","date":null,"permalink":"/tags/azure/","section":"Tags","summary":"","title":"Azure"},{"content":"","date":null,"permalink":"/tags/azuremanagedsql/","section":"Tags","summary":"","title":"Azuremanagedsql"},{"content":"PrÃ©requis # Un compte Azure (ici j\u0026rsquo;utilise mon compte Azure Student) Terraform AzureCLI Un compte de stockage Azure, avec un containeur permettant de stocker le terraform.tfstate Dans mon cas, j\u0026rsquo;ai crÃ©Ã© un groupe de ressource dÃ©diÃ© Ã  mon backend via le portail Azure, puis le compte de stockage dans ce groupe :\nCi ce n\u0026rsquo;est pas dÃ©jÃ  fait, il faut t\u0026rsquo;authentifier Ã  Azure via la commande az login et suivre les instructions.\nObjectif #L\u0026rsquo;objectif de cet article est d\u0026rsquo;expliquer la mise en place d\u0026rsquo;une instance GLPI dans le cloud Azure, et de sa base de donnÃ©e MySQL en utilisant des services managÃ©s proposÃ©s par Microsoft. L\u0026rsquo;intÃ©gralitÃ© du code est disponible ce repo . Cet article est basÃ© sur le dÃ©ploiement de GLPI, mais devrais fonctionner avec n\u0026rsquo;importe quelle application web fonctionnant avec une base de donnÃ©e MySQL.\nInfrastructure cible #Pour dÃ©ployer notre instance de test, nous allons (dans l\u0026rsquo;ordre) dÃ©finir :\nUn groupe de ressource Azure qui va contenir l\u0026rsquo;ensemble des ressources Un VPC (Virtual Network sur Azure) 2 subnets : un dÃ©diÃ© Ã  la base de donnÃ©es, l\u0026rsquo;autre Ã  l\u0026rsquo;application web. Un serveur managÃ© Azure mysql flexible (plan Bs1) Une base de donnÃ©e managÃ©e Azure SQL Un plan App Service Linux (B1) Une App Service Linux Une rÃ¨gle de pare-feu autorisant les instances Azure Ã  accÃ©der Ã  la base de donnÃ©e. La base de donnÃ©e ne sera pas accessible via le web, mais uniquement via notre VPC. En fin de compte, l\u0026rsquo;architecture devrais ressembler Ã  ceci :\nTerraform #Structure #Notre code Terraform se dÃ©compose en 3 fichiers :\nâ”œâ”€â”€ inputs.tf â”œâ”€â”€ main.tf â””â”€â”€ outputs.tf inputs.tfdÃ©finit les variables d\u0026rsquo;entrÃ©es (adressage, nom des ressources etc). inputs.tf outputs.tfpermet d\u0026rsquo;afficher les donnÃ©es nÃ©cessaires pour se connecter Ã  notre app une fois le dÃ©ploiement terminÃ©. outputs.tf main.tf contient l\u0026rsquo;ensemble du code provisionnant l\u0026rsquo;infrastructure. Je vais le dÃ©tailler ci-dessous. Configuration du backend #Au dÃ©but du fichier main.tf, je configure le provider Terraform azurerm, en lui indiquant oÃ¹ stocker le fichier terraform.statepermettant de garder en mÃ©moire la configuration de l\u0026rsquo;infrastructure :\nterraform { backend \u0026#34;azurerm\u0026#34; { resource_group_name = \u0026#34;backend-terraform-rg\u0026#34; storage_account_name = \u0026#34;terraformbackend9809\u0026#34; container_name = \u0026#34;terraform\u0026#34; key = \u0026#34;terraform.tfstate\u0026#34; } required_providers { azurerm = { source = \u0026#34;hashicorp/azurerm\u0026#34; version = \u0026#34;~\u0026gt; 3.47.0\u0026#34; } } required_version = \u0026#34;\u0026gt;= 1.4.0\u0026#34; } provider \u0026#34;azurerm\u0026#34; { features {} } CrÃ©ation du groupe de ressource #Pour crÃ©er des ressources dans Azure, il faut crÃ©er un groupe de ressources :\n# Resource Group resource \u0026#34;azurerm_resource_group\u0026#34; \u0026#34;rg\u0026#34; { name = var.resource_group_name location = var.location } A noter: le code reprends les variables dÃ©finies dans le fichier inputs.tf. Il en va de mÃªme pour les exemples suivants. CrÃ©ation du VPC et des sous-rÃ©seaux #L\u0026rsquo;exemple de code ci-dessous crÃ©e un VPC, et deux sous rÃ©seaux. Un pour la base de donnÃ©e, et un pour l\u0026rsquo;application web :\n# Virtual Network and subnet resource \u0026#34;azurerm_virtual_network\u0026#34; \u0026#34;vnet\u0026#34; { name = var.vnet_name address_space = var.vnet_address_space location = var.location resource_group_name = azurerm_resource_group.rg.name } resource \u0026#34;azurerm_subnet\u0026#34; \u0026#34;mysql_subnet\u0026#34; { name = var.mysql_subnet_name resource_group_name = azurerm_resource_group.rg.name virtual_network_name = azurerm_virtual_network.vnet.name address_prefixes = var.mysql_subnet_address_prefixes service_endpoints = [\u0026#34;Microsoft.Sql\u0026#34;] delegation { name = \u0026#34;vnet-delegation\u0026#34; service_delegation { name = \u0026#34;Microsoft.DBforMySQL/flexibleServers\u0026#34; actions = [ \u0026#34;Microsoft.Network/virtualNetworks/subnets/action\u0026#34; ] } } } resource \u0026#34;azurerm_subnet\u0026#34; \u0026#34;app_service_subnet\u0026#34; { name = var.app_subnet_name resource_group_name = azurerm_resource_group.rg.name virtual_network_name = azurerm_virtual_network.vnet.name address_prefixes = var.app_subnet_address_prefixes delegation { name = \u0026#34;vnet-delegation\u0026#34; service_delegation { name = \u0026#34;Microsoft.Web/serverFarms\u0026#34; actions = [\u0026#34;Microsoft.Network/virtualNetworks/subnets/action\u0026#34;] } } } Les dÃ©lÃ©gations configurÃ©es dans les blocs delegationet service_delegationpermettent de designer ces subnets en tant que cibles pour des ressources PaaS Azure. Serveur de base de donnÃ©e SQL #J\u0026rsquo;ai choisi de dÃ©ployer une base de donnÃ©e de type flexible, en utilisant le plan B_Standard_B1s. C\u0026rsquo;est suffisant pour notre infrastructure, et peu onÃ©reux. Le tableau des tarifs est disponible ici.\nresource \u0026#34;azurerm_mysql_flexible_server\u0026#34; \u0026#34;mysql\u0026#34; { name = var.mysql_server_name location = azurerm_resource_group.rg.location resource_group_name = azurerm_resource_group.rg.name administrator_login = var.mysql_database_admin_username administrator_password = random_password.mysql_password.result backup_retention_days = 5 sku_name = \u0026#34;B_Standard_B1s\u0026#34; delegated_subnet_id = azurerm_subnet.mysql_subnet.id } Le mot de passe administrateur est gÃ©nÃ©rÃ© alÃ©atoirement et sera affichÃ© dans les output aprÃ¨s avoir dÃ©ployÃ© l\u0026rsquo;infrastructure. L\u0026rsquo;option\u0026rsquo; delegated_subnet_id permet d\u0026rsquo;attacher ce serveur au subnet prÃ©cÃ©dement crÃ©Ã©.\nBase de donnÃ©e SQL ## MySQL Database resource \u0026#34;azurerm_mysql_flexible_database\u0026#34; \u0026#34;mysql\u0026#34; { name = var.mysql_database_name resource_group_name = azurerm_resource_group.rg.name server_name = azurerm_mysql_flexible_server.mysql.name charset = \u0026#34;utf8\u0026#34; collation = \u0026#34;utf8_unicode_ci\u0026#34; } Cet extrait de code crÃ©e une base de donnÃ©e dÃ©diÃ©e Ã  GLPI, sur le serveur MySQL managÃ© Azure.\nRÃ¨gles de firewall ## Firewall rule resource \u0026#34;azurerm_mysql_flexible_server_firewall_rule\u0026#34; \u0026#34;fw-mysql\u0026#34; { name = \u0026#34;AllowAzureIPs\u0026#34; resource_group_name = azurerm_resource_group.rg.name server_name = azurerm_mysql_flexible_server.mysql.name start_ip_address = \u0026#34;0.0.0.0\u0026#34; end_ip_address = \u0026#34;0.0.0.0\u0026#34; } Par convention, autoriser les ips 0.0.0.0revient Ã  autoriser les ressources Azure uniquement d\u0026rsquo;accÃ©der Ã  la base de donnÃ©e.\nPlan Azure App Service et application App Service #resource \u0026#34;azurerm_service_plan\u0026#34; \u0026#34;glpi-service-plan\u0026#34; { name = var.glpi_app_service_plan_name resource_group_name = azurerm_resource_group.rg.name location = azurerm_resource_group.rg.location os_type = \u0026#34;Linux\u0026#34; sku_name = \u0026#34;B1\u0026#34; } resource \u0026#34;azurerm_linux_web_app\u0026#34; \u0026#34;glpi-app-service\u0026#34; { name = var.glpi_app_service_name resource_group_name = azurerm_resource_group.rg.name location = azurerm_service_plan.glpi-service-plan.location service_plan_id = azurerm_service_plan.glpi-service-plan.id site_config { always_on = false application_stack { docker_image = \u0026#34;diouxx/glpi\u0026#34; docker_image_tag = \u0026#34;latest\u0026#34; } } } #Connect the Azure App to subnet resource \u0026#34;azurerm_app_service_virtual_network_swift_connection\u0026#34; \u0026#34;app\u0026#34; { app_service_id = azurerm_linux_web_app.glpi-app-service.id subnet_id = azurerm_subnet.app_service_subnet.id } Cet extrait de code dÃ©ploye l\u0026rsquo;application web GLPI au sein d\u0026rsquo;un plan B1. Dans mon exemple, j\u0026rsquo;utilise une image docker GLPI hÃ©bergÃ©e sur le docker hub.\nLe plan B1 est le plan App Service le moins cher permettant d\u0026rsquo;attacher un App Service Ã  un subnet, et ainsi pouvoir communiquer en interne avec la base de donnÃ©e. DÃ©ploiement de l\u0026rsquo;infrastructure #Une fois le code rÃ©digÃ© (ou tÃ©lÃ©chargÃ© directement depuis mon repo ), le dÃ©ploiement s\u0026rsquo;effectue en 3 commandes :\nterraform init terraform plan terraform apply terraform init permet de configurer le backend et rÃ©cupÃ©rer le module azurerm. La commande plan permet de passer en revue les changements apportÃ©s Ã  l\u0026rsquo;infrastructure, et apply de la dÃ©ployer :\nSi tout se passe bien, la sortie de la commande apply te donneras le nÃ©cessaire pour configurer ton app GLPI :\nLe dÃ©ploiement peut prendre du temps, notamment la base de donnÃ©e MySQL (+/- 10 minutes dans mon cas). L\u0026rsquo;App Service peut aussi mettre un certain temps avant d\u0026rsquo;Ãªtre accessible depuis l\u0026rsquo;adresse, le temps que l\u0026rsquo;image docker soit dÃ©ployÃ©e. DerniÃ¨res Ã©tapes de configuration #Dans l\u0026rsquo;interface d\u0026rsquo;installation de GLPI, saisir les informations donnÃ©es dans l\u0026rsquo;output terraform. Si tu as utilisÃ© mon fichier input.tf, l\u0026rsquo;utilisateur SQL sera glpi. La premiÃ¨re tentative de connexion donnera cette erreur :\nPour la corriger, il suffit de se rendre dans les paramÃ¨tres de la BDD sur le portail Azure, et de passer le paramÃ¨tre require_secure_transportsur OFF:\nIl est normalement dÃ©sormais possible d\u0026rsquo;installer GLPI sur la base de donnÃ©e crÃ©Ã©e pour l\u0026rsquo;occasion Ã  l\u0026rsquo;aide de Terraform:\nVÃ©rifications #Une fois l\u0026rsquo;application installÃ©e, on se connecte Ã  l\u0026rsquo;aide des identifiants par dÃ©faut (glpi/glpi):\nC\u0026rsquo;est fonctionnel!\nEn raison de la faible puissance de la base de donnÃ©e managÃ©e choisie, l\u0026rsquo;affichage peut Ãªtre trÃ¨s long dans l\u0026rsquo;application. Si c\u0026rsquo;est inutilisable, ne pas hÃ©siter Ã  augmenter la taille de l\u0026rsquo;instance SQL. Suppression de l\u0026rsquo;infrastructure #Un simple terraform destroydÃ©truira l\u0026rsquo;ensemble des ressources crÃ©es. A ne pas oublier, le coÃ»t mensuel de l\u0026rsquo;App Service peut Ãªtre Ã©levÃ©.\n","date":"13 mars 2023","permalink":"/blog/informatique/azure-appservices-terraform/","section":"Blog","summary":"En prenant pour example GLPI, je t\u0026rsquo;explique comment dÃ©ployer une application web complÃ¨te as code en utilisant App Services","title":"DÃ©ployer GLPI 10 sur Azure App Services avec une base de donnÃ©e managÃ©e via Terraform"},{"content":"","date":null,"permalink":"/tags/glpi/","section":"Tags","summary":"","title":"Glpi"},{"content":"","date":null,"permalink":"/tags/latex/","section":"Tags","summary":"","title":"Latex"},{"content":"","date":null,"permalink":"/categories/markdown/","section":"Categories","summary":"","title":"Markdown"},{"content":"","date":null,"permalink":"/tags/markdown/","section":"Tags","summary":"","title":"Markdown"},{"content":"","date":null,"permalink":"/tags/pandoc/","section":"Tags","summary":"","title":"Pandoc"},{"content":"","date":null,"permalink":"/tags/pdf/","section":"Tags","summary":"","title":"Pdf"},{"content":"Le language Markdown # Attention Cet article part du principe que vous Ãªtes dÃ©jÃ  Ã  l\u0026rsquo;aise avec la rÃ©daction en Markdown. Si ce n\u0026rsquo;est pas le cas, un tutorial interactif assez bien fait est disponible ici Markdown est un language de balisage facile Ã  utiliser et Ã  lire. Je m\u0026rsquo;en suis beaucoup servi lors de mes Ã©tudes pour prendre des notes, car Markdown permet d\u0026rsquo;obtenir un rÃ©sultat propre trÃ¨s rapidement avec un simple Ã©diteur de texte. DiffÃ©rents logiciels sont compatible avec Markdown, permettant d\u0026rsquo;organiser ses notes et d\u0026rsquo;obtenir un aperÃ§u en temps rÃ©el lors de l\u0026rsquo;Ã©dition. Personnellement, j\u0026rsquo;utilise Bear sous MacOS:\nInterface principal de Bear, avec le texte formatÃ© en utilisant Markdown Markdown est aussi extrÃªmement rÃ©pandu dans le monde de l\u0026rsquo;informatique, la majoritÃ© des fichiers README Ã©tant rÃ©digÃ© en utilisant la syntaxe Markdown.\nLes limites du Markdown #J\u0026rsquo;ai tellement pris l\u0026rsquo;habitude de rÃ©diger mes documentations et notes en Markdown, que Ã§a soit sur VS Code ou sur des applications type Bear, que l\u0026rsquo;utilisation de Word me paraÃ®t dÃ©sormais extrÃªmement lourde et contre-productive. Le souci, c\u0026rsquo;est que par dÃ©faut, Markdown n\u0026rsquo;est pas prÃ©vu pour gÃ©nÃ©rer des documents imprimables. En effet, c\u0026rsquo;est uniquement un language de balisage, conÃ§u pour Ãªtre interprÃ©tÃ© et affichÃ© Ã  l\u0026rsquo;Ã©cran.\nGÃ©nÃ©rer un document type \u0026ldquo;MÃ©moire\u0026rdquo; depuis des fichiers Markdown #Je me suis basÃ© sur ce repo pour crÃ©er un template (en FranÃ§ais) permettant de gÃ©nÃ©rer un document adaptÃ© Ã  mes rendus d\u0026rsquo;Ã©cole. ConcrÃ¨tement, il s\u0026rsquo;agit ici d\u0026rsquo;utiliser Pandoc associÃ© Ã  un template LaTeX permettant de gÃ©nÃ©rer le document. Rassurez-vous, il n\u0026rsquo;est pas nÃ©cessaire d\u0026rsquo;apprendre LaTeX pour utiliser ce projet, seule certaines commandes suffisent.\nÃ‰tape 1 #RÃ©cupÃ©rer sur mon GitHub le template :\ngit clone https://github.com/fabienchevalier/phd_thesis_markdown.git Pandoc nÃ©cessite LaTeX pour gÃ©nÃ©rer des PDF. En fonction de votre OS, il faut donc installer une distribution LaTeX:\nLinux: sudo apt-get install textlive Windows: voir du cÃ´tÃ© de MiKTex MacOS: via homebrew avec brew install --cask mactex Par la suite, mettre Ã  jour sa distribution LaTeX:\nsudo tlmgr update --self Ã‰tape 2 #Installer Pandoc dans un environnement Python sÃ©parÃ©. Personnellement, j\u0026rsquo;utilise miniconda sous MacOS (disponible aussi sur Linux/Windows):\nbrew install miniconda #apt-get install miniconda devrais fonctionner sous Ubuntu/Debian conda create -n phd -y python=3.7 pandoc conda activate phd Les autres dÃ©pendances peuvent Ãªtre installÃ©es automatiquement via le Makefile proposÃ© :\nmake install Ã‰tape 3 #Par la suite, dÃ©poser les fichiers md dans le dossier source et executer make pdf pour gÃ©nÃ©rer le document au format PDF. Ne pas oublier de modifier le fichier metadata.yml dans le dossier source en fonction des besoins! Un exemple de rendu est disponible ici.\n","date":"7 fÃ©vrier 2023","permalink":"/blog/informatique/markdown-to-pdf-latex/","section":"Blog","summary":"Comment gÃ©nÃ©rer un document type \u0026ldquo;MÃ©moire\u0026rdquo; au format PDF depuis des fichiers Markdown en utilisant LaTeX et Pandoc","title":"RÃ©diger son mÃ©moire universitaire en Markdown"},{"content":" RÃ©alisÃ© dans le cadre de mes deux annÃ©es de BTS, ce portofolio rÃ©sume les travaux entrepris lors de ces deux annÃ©es d\u0026rsquo;Ã©tude. EntiÃ¨rement rÃ©alisÃ© en Markdown, ce site web est hÃ©bergÃ© par GitHub et Ã  Ã©tÃ© construit Ã  partir du template Jekyll Minimal Mistake.\nhttps://fchevalier.net/bts\n","date":"1 septembre 2022","permalink":"/projets/bts_sio/","section":"Projets","summary":"RÃ©alisÃ© dans le cadre de mes deux annÃ©es de BTS, ce portofolio rÃ©sume les travaux entrepris lors de ces deux annÃ©es d\u0026rsquo;Ã©tude.","title":"Portofolio BTS SIO SISR"}]
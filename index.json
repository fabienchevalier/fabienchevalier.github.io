[{"content":"Hi 👋 #🏢 DevOps @Claranet France\n","date":null,"permalink":"/","section":"/home","summary":"Hi 👋 #🏢 DevOps @Claranet France","title":"/home"},{"content":" Réalisé dans le cadre de mon année de Bachelor, le Hackaton à pour but de proposer une solution informatique en une semaine. Un jury juge ensuite la solution la plus convaincante et désigne les vainqueurs.\nL\u0026rsquo;ensemble du projet que j\u0026rsquo;ai réalisé est disponible en open-source\nsur GitLab\nEtant encore étudiant et loin d\u0026rsquo;être un spécialiste K8S, le projet présenté sur cette page comporte forcément de grosses imprécisions et d\u0026rsquo;énormes failles de sécurité. Si par hasard un DevOps confirmé tombe sur ces lignes, n\u0026rsquo;hésites pas à me contacter via la section commentaire ou via email, j\u0026rsquo;aurais quelques questions à te poser 😉 Introduction #Lors de ce hackathon, mon équipe à choisi pour sujet Solution Libre. Il s\u0026rsquo;agissait de :\ndévelopper le frontend d\u0026rsquo;une application à destination de formateurs et de leurs élèves développer l\u0026rsquo;architecture qui permettra in fine d\u0026rsquo;héberger l\u0026rsquo;application, son backend et sa base de données proposer une CI/CD permettant de mettre en place une livraison et une intégration continue de l\u0026rsquo;applicatif et son infrastructure Le back-end étant fourni par l\u0026rsquo;école, mon travail ici se limitait à son intégration ainsi qu\u0026rsquo;a celle du front-end.\nContexte et travail d\u0026rsquo;équipe #L\u0026rsquo;ensemble des étudiants de l\u0026rsquo;école sont réunis en équipe de 10, tout niveaux et spécialités confondues. Toute la difficulté étant de coordonner l\u0026rsquo;ensemble de l\u0026rsquo;équipe afin de livrer un produit fonctionnel.\nLimites et difficultés #Malheureusement, l\u0026rsquo;équipe dont j\u0026rsquo;ai fais partie n\u0026rsquo;as pas sû délivrer un front-end dans les temps impartis. Cela dit, de mon côté j\u0026rsquo;ai pu architecturer l\u0026rsquo;ensemble de l\u0026rsquo;infrastructure ainsi que la pipeline CI/CD et obtenir un POC fonctionnel. Il ne me manquais plus qu\u0026rsquo;un front-end afin d\u0026rsquo;obtenir le résultat demandé, dommage 😥!\nCependant, j\u0026rsquo;ai pu tester le fonctionnement du back-end sur l\u0026rsquo;architecture ainsi déployée. C\u0026rsquo;est d\u0026rsquo;ailleurs l\u0026rsquo;objet de cet article : présenter ma solution sur la partie DevOps 😎.\nSchéma d\u0026rsquo;architecture #J\u0026rsquo;ai réalisé un petit brouillon de l\u0026rsquo;architecture que j\u0026rsquo;ai souhaité mettre en place sur le cloud Azure :\nOn à donc:\nUn cluster Kubernetes managé sur le cloud Azure (AKS) 3 namepsaces : un pour le front, un pour le back et le dernier pour le monitoring et la gestion des logs Lors de la réalisation, j\u0026rsquo;ai regroupé tout mes pods K8S au sein du même namespace par manque de temps lors du développement de la partie Terraform. La partie monitoring est aussi inachevée et ne figure pas sur le repo. Cela dit, il n\u0026rsquo;est pas exclu que je m\u0026rsquo;y penche dans le cadre d\u0026rsquo;un article de blog dans un futur proche. En l\u0026rsquo;état, l\u0026rsquo;architecture disponible sur le lien GitLab donnée en préambule permet uniquement de requêter l\u0026rsquo;API du backend. De plus, comme indiqué sur le schéma la base de donnée utilisée est hébergée dans un pod. J\u0026rsquo;aurais préféré mettre en place une base de donnée managée mais n\u0026rsquo;étant pas encore très à l\u0026rsquo;aise avec Kubernetes sur Azure, j\u0026rsquo;ai préféré aller au plus simple. Le but de cet article reste pour moi l\u0026rsquo;occasion de garder une sorte de documentation sur le projet réalisé. A ne pas utiliser en production donc 😉.\nInfrastructure As Code #Contexte #L\u0026rsquo;enjeu étant de provisioner tout cela as code, j\u0026rsquo;ai déployé cette architecture avec Terraform. Le repo contenant l\u0026rsquo;infrastructure est construit comme ceci:\n└── terraform ├── aks.tf ├── environment │ └── dev │ └── variables.tfvars ├── kubernetes.tf ├── main.tf ├── modules │ └── kubernetes │ ├── main.tf │ ├── outputs.tf │ └── variables.tf ├── outputs.tf ├── rg.tf ├── variables.tf └── vpc.tf Cette architecture est dès le départ conçue pour être en mesure de déployer des environments ISO dev/pprd/prod. Cela permet au développeur d\u0026rsquo;être en mesure (en théorie) de tester son code en amont sur des architectures identiques avant de déployer l\u0026rsquo;applicatif en production.\nBackend et providers #Dans le fichier main.tf, en début de code on retrouve la déclaration du backend, ainsi que les providers requis:\nterraform { backend \u0026#34;azurerm\u0026#34; { resource_group_name = \u0026#34;backend-terraform-rg\u0026#34; storage_account_name = \u0026#34;backendhackatonsdv\u0026#34; container_name = \u0026#34;terraform-state\u0026#34; key = \u0026#34;terraform.tfstate\u0026#34; } required_providers { azurerm = { source = \u0026#34;hashicorp/azurerm\u0026#34; version = \u0026#34;3.63.0\u0026#34; } kubernetes = { source = \u0026#34;hashicorp/kubernetes\u0026#34; version = \u0026#34;2.21.1\u0026#34; } } } provider \u0026#34;azurerm\u0026#34; { features {} skip_provider_registration = true } provider \u0026#34;kubernetes\u0026#34; { host = module.aks.host client_certificate = base64decode(module.aks.client_certificate) client_key = base64decode(module.aks.client_key) cluster_ca_certificate = base64decode(module.aks.cluster_ca_certificate) } Le backend, permettant de stocker le fichier tfstate consiste en un compte de stockage Azure hébergeant ce même fichier. Dans le cas de ce lab, j\u0026rsquo;ai utilisé un compte de stockage préalablement existant sur mon compte Azure.\nJ\u0026rsquo;utilise les providers azure_rm et kubernetes permettant de respectivement :\nintéragir avec mon compte Azure intéragir avec le cluster Kubernetes déployé J\u0026rsquo;utilise un compte Azure Students fourni par l\u0026rsquo;école pour ce lab. Terraform refusais systématiquement d\u0026rsquo;apply mon infrastructure en raison d\u0026rsquo;erreurs de droits. L\u0026rsquo;option skip_provider_registration = true m\u0026rsquo;as permis de débloquer la situation. Le provider kubernetes nécessite une configuration permettant de se connecter au cluster afin d\u0026rsquo;y déployer les ressources. Ici, je fournis les valeurs dynamiquement à partir du module aks que je decris plus loin dans l\u0026rsquo;article.\nTerraform et modules #Pour une meilleure lisibilité, je préfère séparer chaque ressources déployées en un fichier distinct. Cela évite de se retrouver avec un gros fichier de plusieurs centaines de lignes devenant vite illisible. Terraform permet ensuite d\u0026rsquo;organiser son code en modules. Un module est un ensemble de fichiers Terraform stockés dans un dossier. Cela permet d\u0026rsquo;éviter les répétitions dans le code et la méthode de fonctionnement peut être comparable à une fonction dans un programme informatique. Ici j\u0026rsquo;ai donc :\naks.tf -\u0026gt; Crée un cluster Kubernetes managé via le module Azure/aks/azurerm kubernetes.tf -\u0026gt; Intéragit avec le cluster Kubernetes. Ce fichier appelle un module que j\u0026rsquo;ai développé et stocké dans le répertoire modules rg.tf -\u0026gt; Le groupe de ressource Azure contenant l\u0026rsquo;ensemble des instances vpc.tf -\u0026gt; La configuration réseau de l\u0026rsquo;infrastructure Pour finir, dans le fichier variables.tf, je déclare les variables qui seront utilisées pour déployer l\u0026rsquo;infrastructure. Ces variables sont fournies par le fichier variables.tfvars, qui diffère en fonction de l\u0026rsquo;environement de production choisi. Ainsi, au plan ou apply il suffira de rajouter le flag -var-file=environment/dev/variables.tfvars afin de définir l\u0026rsquo;environment choisi. Dans ce lab, il s\u0026rsquo;agit de l\u0026rsquo;environment de dev.\nCI/CD #Containerisation #Pour être en mesure de déployer l\u0026rsquo;infrastructure sur Kubernetes, il m\u0026rsquo;a fallu au préalable containairiser le back-end dans une image Docker prête à être stocké sur un registre d\u0026rsquo;images (ici, j\u0026rsquo;utilise celui de GitLab). La méthode aurais été similaire pour le front-end. J\u0026rsquo;ai donc écris un Dockerfile :\n# Base Golang Image FROM golang:latest # Setup working directory WORKDIR /usr/src/osf-core # Copy source code to COPY . /usr/src/osf-core # Install Git and NodeJS RUN curl -sL https://deb.nodesource.com/setup_16.x | bash - RUN apt-get install -y nodejs npm # Install NPM dependencies RUN npm install -g @marp-team/marp-core \\ \u0026amp;\u0026amp; npm install -g markdown-it-include \\ \u0026amp;\u0026amp; npm install -g markdown-it-container \\ \u0026amp;\u0026amp; npm install -g markdown-it-attrs # Install Go Library \u0026amp; Swagger RUN cd /usr/src/osf-core \u0026amp;\u0026amp; go get golang.org/x/text/transform \\ \u0026amp;\u0026amp; go get golang.org/x/text/unicode/norm \\ \u0026amp;\u0026amp; go install github.com/swaggo/swag/cmd/swag@v1.8.12 # Init Swagger RUN cd /usr/src/osf-core \u0026amp;\u0026amp; swag init --parseDependency --parseInternal # Export ports EXPOSE 8000/tcp EXPOSE 443/tcp EXPOSE 80/tcp # Launch the API CMD [\u0026#34;go\u0026#34;, \u0026#34;run\u0026#34;, \u0026#34;/usr/src/osf-core/main.go\u0026#34;] J\u0026rsquo;ai tenté d\u0026rsquo;utiliser les fichiers packages.json et go.sum/go.mod afin d\u0026rsquo;installer les dépendances directement depuis ces fichiers, mais la génération de mon image plantait. De plus, j\u0026rsquo;ai du forcer la version de swagger en 1.8.12 car un problème de compatibilité m\u0026rsquo;empêchais de générer la documentation Swagger.\nCe Dockerfile est utilisé dans la CI afin de générer dynamiquement l\u0026rsquo;image destiné à être poussée dans le cluster K8S.\nCI applicative #La CI applicative est très basique pour ce lab mais il y a quelques spécificités. J\u0026rsquo;utilise une image docker in docker, car pour builder l\u0026rsquo;image applicative il est nécessaire d\u0026rsquo;executer des commandes docker dans docker (plus d\u0026rsquo;infos ici). Pour fonctionner correctement, le build d\u0026rsquo;une image Docker par le runner nécessite d\u0026rsquo;ajouter ces lignes en début de CI :\nimage: docker:20.10.16 services: - docker:20.10.16-dind variables: DOCKER_TLS_CERTDIR: \u0026#34;/certs\u0026#34; Sans cela, docker serais incapable d\u0026rsquo;accéder à Internet à l\u0026rsquo;intérieur du container.\nLa CI consiste en 3 stages :\ntest deploy trigger_deploy_to_terraform 2 variables sont à renseigner manuellement : ENV et TAG:\nENV: value: \u0026#34;dev\u0026#34; description: \u0026#34;On wich env the image should be deployed\u0026#34; TAG: value: \u0026#34;latest\u0026#34; description: \u0026#34;Version of the image\u0026#34; Ces variables sont utilisées par la suite pour définir sur quel environment déployer l\u0026rsquo;image, et permet à Terraform de récupérer le chemin vers l\u0026rsquo;image à pousser sur le cluster. Le script qui va créer l\u0026rsquo;image est très simple :\ndeploy: stage: deploy script: - docker login -u $CI_REGISTRY_USER -p $CI_REGISTRY_TOKEN $DOCKER_REGISTRY_URL - docker build -t registry.gitlab.com/sdv-open-course-factory/ocf-core/${ENV}-backend:${TAG} . - docker push registry.gitlab.com/sdv-open-course-factory/ocf-core/${ENV}-backend:${TAG} Le nom et le chemin de l\u0026rsquo;image sera renseigné automatiquement en fonction des données entrées par le dev.\nPour finir, le dernier stage déclenche la CI situées sur le repo Terraform, en poussant la variable TAG permettant à Terraform de pousser la bonne image du backend sur le cluster Kubernetes:\nNew job to trigger the other project\u0026#39;s CI trigger_deploy_to_terraform: image: curlimages/curl stage: trigger_deploy_to_terraform script: - curl -X POST --fail -F token=$CI_TRIGGER_TOKEN -F \u0026#34;ref=main\u0026#34; -F \u0026#34;variables[TAG]=$TAG\u0026#34; https://gitlab.com/api/v4/projects/47370418/trigger/pipeline needs: - deploy Je n\u0026rsquo;ai pas encore variabilisé l\u0026rsquo;environement à ce niveau. Pour le moment, ma CI est uniquement en mesure de déployer sur dev. Encore une fois, question de priorités niveau timing. Je cherchais surtout à avoir quelque chose de fonctionnel au plus vite. Ainsi, seul la variable TAG est envoyée à Terraform, permettant de retrouver l\u0026rsquo;image. CI infrastructure #La CI d\u0026rsquo;infrastructure peut soit être déclenchée par le job trigger de la CI applicative, soit manuellement. On retrouve les stages classiques :\nvalidate plan apply destroy J\u0026rsquo;utilise ici l\u0026rsquo;image hashicorp/terraform:latest, m\u0026rsquo;évitant d\u0026rsquo;installer Terraform à chaque déploiement sur le runner.\nAu niveau des variables :\nvariables: TF_ROOT: ${CI_PROJECT_DIR}/terraform/ TF_ENVIRONMENT: \u0026#34;dev\u0026#34; # Définissez l\u0026#39;environnement souhaité ici (par exemple, dev, preprod, prod) TF_DESTROY: description: Destroy Terraform resources value: \u0026#34;false\u0026#34; Comme dit précédemment, l\u0026rsquo;environement et codé en dur sur dev dans mon lab La variable TF_DESTROY me permet de détruire l\u0026rsquo;infrastructure via la CI. Le stage destroy ne s\u0026rsquo;execute seulement ci la valeur est changée pour true:\nterraform_destroy: stage: destroy script: terraform destroy -auto-approve -var-file=environment/${TF_ENVIRONMENT}/variables.tfvars -var=\u0026#34;img_tag=${TAG}\u0026#34; rules: - if: $TF_DESTROY == \u0026#34;true\u0026#34; when: always La partie plan contient un simple script bash qui va tester l\u0026rsquo;existence de la variable TAG récupérée depuis la CI applicative :\nif [ -n \u0026#34;$TAG\u0026#34; ]; then terraform plan -var-file=environment/${TF_ENVIRONMENT}/variables.tfvars -var \u0026#34;img_tag=${TAG}\u0026#34; else echo \u0026#34;Nothing to plan\u0026#34; fi J\u0026rsquo;utilise le flag -var-file=environment/${TF_ENVIRONMENT}/variables.tfvars afin de déterminer dynamiquement sur quel environenment déployer. Puis le flag -var \u0026quot;img_tag=${TAG} pour déterminer l\u0026rsquo;image Docker à utiliser sur le même principe.\nLe flag -var \u0026quot;img_tag=${TAG} permet de définir la variable Terraform contenue dans le variables.tf à la racine ainsi que dans le module Kubernetes:\nvariable \u0026#34;img_tag\u0026#34; { description = \u0026#34;Image tag\u0026#34; type = string } L\u0026rsquo;adresse permettant de pointer vers l\u0026rsquo;image est ensuite reconstruite comme ceci lors de la création du déploiement Kubernetes:\nspec { container { image = \u0026#34;registry.gitlab.com/sdv-open-course-factory/ocf-core/${var.env}-backend:${var.img_tag}\u0026#34; name = \u0026#34;ocf-core-backend\u0026#34; port { container_port = 80 } port { container_port = 443 } port { container_port = 8000 } Conclusion #J\u0026rsquo;ai eu au final à peu près 4 jours pour réaliser toute cette architecture. C\u0026rsquo;est assez court, mais le POC est fonctionnel et permet d\u0026rsquo;accéder à la doc swagger du backend et de manipuler l\u0026rsquo;API. Dans l\u0026rsquo;idée, pour vraiment déployer ce lab en production il faudrais:\nBien sûr y intégrer un front-end Retravailler la CI pour intégrer l\u0026rsquo;image du front-end Intégrer toute la partie monitoring et gestion des logs, c\u0026rsquo;est super important Utiliser une BDD managée Azure au lieu d\u0026rsquo;une image PostegrSQL dans le cluster Séparer chaque services sur des namespaces différents Niveau sécurité, placer le loadbalancer sur un subnet séparé. Mettre en place un bastion sur un VPC différent afin d\u0026rsquo;être en mesure de requêter l\u0026rsquo;API K8S via kubectl. Ici, ⚠️ l\u0026rsquo;API est exposée au web ⚠️ Je pense ré-utiliser mon architecture pour tenter de réaliser le front-end. J\u0026rsquo;en profiterais pour consolider le tout par rapport à la liste ci-dessus. Cela fais aussi quelque temps que je suis intéressé par les technologies de dev orienté front-end (React) et n\u0026rsquo;ayant pas ou très peu d\u0026rsquo;expérience en JavaScript, ce serais l\u0026rsquo;occasion. Affaire à suivre donc\u0026hellip;\n","date":"3 juillet 2023","permalink":"/projets/hackaton_sdv/","section":"Projets","summary":"Réalisé dans le cadre de mon année de Bachelor, le Hackaton à pour but de proposer une solution informatique en une semaine.","title":"Hackaton Sup De Vinci 2023"},{"content":" Cette page rassemble l\u0026rsquo;ensemble de mes projets scolaires et personnels réalisés en classe ou sur mon temps libre ","date":null,"permalink":"/projets/","section":"Projets","summary":" Cette page rassemble l\u0026rsquo;ensemble de mes projets scolaires et personnels réalisés en classe ou sur mon temps libre ","title":"Projets"},{"content":"","date":null,"permalink":"/categories/aws/","section":"Categories","summary":"","title":"Aws"},{"content":"","date":null,"permalink":"/tags/awscli/","section":"Tags","summary":"","title":"Awscli"},{"content":" C\u0026rsquo;est ici que je publie mes articles quand le temps me le permet. 📌 Articles récents #","date":null,"permalink":"/blog/","section":"Blog","summary":"C\u0026rsquo;est ici que je publie mes articles quand le temps me le permet.","title":"Blog"},{"content":"","date":null,"permalink":"/categories/","section":"Categories","summary":"","title":"Categories"},{"content":"","date":null,"permalink":"/tags/cloud/","section":"Tags","summary":"","title":"Cloud"},{"content":"","date":null,"permalink":"/tags/cloudformation/","section":"Tags","summary":"","title":"Cloudformation"},{"content":"","date":null,"permalink":"/tags/ec2/","section":"Tags","summary":"","title":"Ec2"},{"content":"","date":null,"permalink":"/tags/gaming/","section":"Tags","summary":"","title":"Gaming"},{"content":"Introduction #Le cloud-gaming se démocratise de plus en plus notamment par le biais d\u0026rsquo;acteurs tel que OVH avec son offre Shadow, ou encore NVDIA avec GeForce Now. Je me suis toujours demandé par quel procédé il serais possible de créer mon propre serveur cloud dédié au jeu, et surtout si c\u0026rsquo;était viable. Après quelques recherches, j\u0026rsquo;ai découvert quelques pistes assez intéressantes, notament du côté d\u0026rsquo;AWS qui propose des instances de calcul avec GPU plutôt abordables : les instances G4.\nAttention à la facture si tu oublies d\u0026rsquo;éteindre ou de supprimer ton instance après avoir déroulé l\u0026rsquo;article! Cet article te permettra de monter ta propre VM GPU, tout en te faisant découvrir aws cli et CloudFormation, l\u0026rsquo;outil d\u0026rsquo;IAC proposé par AWS.\nLes types d\u0026rsquo;instances G4 #AWS propose deux architectures, l\u0026rsquo;une est basée sur les puces GPU NVIDIA T4 (g4dn), l\u0026rsquo;autre sur des puces AMD Radeon Pro V520 (g4ad). Ci-dessous un tableau comparatif du rapport prix/performances des différentes instances :\nInstance Type 3DMark Score On-demand Price (us-east-1, USD, 02/23) Price-performance (3DMark points / $) g4dn.xlarge 4300 $0.71 6056 g4dn.2xlarge 4800 $1.12 4286 g4dn.4xlarge 6000 $1.94 3093 g4ad.xlarge 5100 $0.56 9107 g4ad.2xlarge 6600 $0.91 7253 g4ad.4xlarge 7600 $1.60 4750 g5.xlarge 6800 $1.19 5714 g5.2xlarge 10200 $1.58 6456 g5.4xlarge 13000 $2.36 5508 Source : https://github.com/aws-samples/cloud-gaming-on-ec2-instances\nPour cet article, je me base sur une instance AMD g4ad.xlarge, largement suffisante pour mes besoins.\nPrérequis # Un compte AWS awscli d\u0026rsquo;installé et de configuré (voir la doc AWS) Assez de quota sur ton compte AWS pour provisionner des instances GPU G4 (j\u0026rsquo;explique plus bas comment en obtenir) A noter: Il n\u0026rsquo;y a pas de free tier proposé par AWS sur les instances GPU. Il est essentiel de bien éteindre ou détruire ta VM une fois la session terminée afin éviter d\u0026rsquo;être facturé pour des heures inutilisées\u0026hellip; Infrastructure cible #Mon template CloudFormation permet de générer :\nUn VPC dédié Un subnet public Une instance EC2 g4adn.xlarge avec le système sur un disque SSD, avec un volume HDD (st1) monté pour stocker les jeux Je me suis basé sur l\u0026rsquo;ami Microsoft Windows Server 2019 with AMD Radeon Pro Driver proposée gratuitement par AWS pour générer mon template. Elle est préconfigurée avec les drivers AMD préinstallés (pratique).\nLes disques st1 sont relativement lents, mais peu cher. J\u0026rsquo;ai choisi cette option ici afin de tester ma config, mais il est conseillé d\u0026rsquo;utiliser un stockage SSD pour jouer confortablement. Déploiement #Quotas de services #Les instances G4 AWS nécessitent des quotas pour être provisionnées. Par défaut, sur ce type d\u0026rsquo;instance, le quota est de 0. Si tu essayes de provisionner une machine G4 en ayant 0 quota, ça ne marchera pas. Pour ce faire, connecte toi à la console AWS et tape service quotas dans la barre de recherche et choisi EC2 :\nDans la barre de recherche sur l\u0026rsquo;écran suivant, tape \u0026ldquo;G4\u0026rdquo; et suit les instructions pour augmenter le quota. Le quota correspond au nombres de vCPU allouées à une instance. Pour la g4ad.xlarge, il nous faut 4 vCPU. Changer la valeur du quota par 4, et envoyer.\nIl n\u0026rsquo;est pas possible d\u0026rsquo;obtenir des quotas pour des host dedicated avec un compte AWS personnel qui a peu servi. Après avoir fait la demande, le service client d\u0026rsquo;AWS la refusera, et te proposera des quotas pour des instances de type spot ou on-demand. J\u0026rsquo;ai demandé d\u0026rsquo;avoir accès aux instances on-demand suite à ma première requête, celles-ci furent disponible le lendemain. Déploiement de l\u0026rsquo;EC2 #Si ce n\u0026rsquo;est pas déjà fais, authentifie toi à awscli. Si tu as besoin d\u0026rsquo;aide, la doc d\u0026rsquo;Amazon est vraiment bien faite.\nÉtant sous MacOS, les commandes suivantes devraient fonctionner sous Linux et MacOS. Si tu es sous Windows, je te conseille de passer via WSL pour bénéficier d\u0026rsquo;un shell bash. Mon template utilise une KeyPair appelée CloudGamingKeyPair permettant de decrypter le mot de passe administrateur de l\u0026rsquo;instance. Pour la créer :\nmkdir ~/.ssh/aws-private-keys \u0026amp;\u0026amp; cd ~/.ssh/aws-private-keys aws ec2 create-key-pair \\ --key-name CloudGamingKeyPair \\ --query \u0026#39;KeyMaterial\u0026#39; \\ --region eu-west-2 \\ --output text \u0026gt; CloudGamingKeyPair.pem Clone mon repo et ce rendre dans le dossier contenant le template :\ngit clone https://github.com/fabienchevalier/ec2-cloudgaming \u0026amp;\u0026amp; cd cloudformation Ouvre le template avec un editeur de texte, et modifie les lignes 60 et 64 en remplacant l\u0026rsquo;adresse par ton adresse IP.\nSecurityGroupIngress: - IpProtocol: tcp FromPort: 8443 #NICE DVC Server ToPort: 8443 CidrIp: 0.0.0.0/0 #Replace that with your IP address (mask should be /32) - IpProtocol: tcp FromPort: 3389 ToPort: 3389 CidrIp: 0.0.0.0/0 #Repl 0.0.0.0/0 autorise n\u0026rsquo;importe quelle adresse sur le port RDP et NICE DCV Server, ce qui n\u0026rsquo;est pas souhaitable.\nTip: tu peux récupérer ton adresse ip public via curl ifconfig.me Déploie le template CloudFormation :\naws --region eu-west-2 cloudformation deploy \\ --template deploy-cloud-gaming-ec2.cfn.yaml \\ --stack-name CloudGamingStack Il est possible de suivre l\u0026rsquo;avancement de la création de la stack sur la console AWS, via la page CloudFormation : Une fois l\u0026rsquo;instance déployée, il faut récupérer le mot de passe Administrateur permettant une première connexion via le protocole RDP.\nOn récupère l\u0026rsquo;ID de l\u0026rsquo;instance crée :\naws --region eu-west-2 ec2 describe-instances \\ --filters \u0026#34;Name=tag:Name,Values=CloudGamingInstance\u0026#34; \\ --query \u0026#39;Reservations[].Instances[].[InstanceId]\u0026#39; \\ --output text Copier l\u0026rsquo;ID généré puis :\naws ec2 get-password-data --instance-id i-1234567890abcdef0 \\ --priv-launch-key ~/.ssh/aws-private-keys/CloudGamingKeyPair.pem Le mot de passe Administrateur se situe dans le champ PasswordData de l\u0026rsquo;output, par exemple :\n{ \u0026#34;InstanceId\u0026#34;: \u0026#34;i-1234567890abcdef0\u0026#34;, \u0026#34;Timestamp\u0026#34;: \u0026#34;2013-08-30T23:18:05.000Z\u0026#34;,z \u0026#34;PasswordData\u0026#34;: \u0026#34;\u0026amp;ViJ652e*u\u0026#34; } On y est presque!\nConfiguration de l\u0026rsquo;instance #Disque dur #Une première connexion via RDP est nécessaire afin de configurer quelques derniers détails. Pour ce faire, lance un client RDP (Microsoft Remote Desktop sous MacOS pour ma part) et connecte toi à l\u0026rsquo;instance via son IP publique.\nUtilise les informations de login récupérées auparavant (login Administrator, mot de passe donné par la commande ec2 get-password-data) pour te connecter. Cherches disk manager dans la barre de recherche Windows, et formate le disque créé par le template CloudFormation comme ceci :\nChoisir l\u0026rsquo;option New Simple Volume, et formater l\u0026rsquo;ensemble de l\u0026rsquo;espace disque disponible.\nServeur Nice DCV #Il nous reste à déployer le serveur Nice DCV permettant de streamer sans latences depuis l\u0026rsquo;instance EC2. Pour ce faire, il faudra se connecter une première fois en RDP via l\u0026rsquo;adresse IP publique de l\u0026rsquo;EC2 afin d\u0026rsquo;executer ce script PowerShell :\n# Set TLS 1.2 for Invoke-RestMethod [Net.ServicePointManager]::SecurityProtocol = [Net.SecurityProtocolType]::Tls12 # Set the download link to Nice DCV Server (64-bit installer) $downloadUrl = \u0026#34;https://d1uj6qtbmh3dt5.cloudfront.net/nice-dcv-server-x64-Release.msi\u0026#34; # Set the path for our download, which will be in the temp directory $installerFile = \u0026#34;nice-dcv-server-x64-Release.msi\u0026#34; $installerDownloadPath = (Join-Path $env:TEMP $installerFile) # Set the default owner to the current user $installerOwner = [Environment]::UserName # Set Install Command Expression $msiExpression = \u0026#34;msiexec.exe /i $installerDownloadPath AUTOMATIC_SESSION_OWNER=$installerOwner ADDLOCAL=ALL /quiet /norestart /l*v dcv_install_msi.log\u0026#34; # Download the file Invoke-Webrequest $downloadUrl -UseBasicParsing -OutFile $installerDownloadPath # Install Invoke-Expression $msiExpression Pour ce faire, copie/colle ce script dans le notepad de l\u0026rsquo;instance, et enregistre le fichier sur le bureau :\nPuis, clic droit sur l\u0026rsquo;icône, puis Run with PowerShell. Attendre une ou deux minutes après que la fenêtre se ferme, le temps que le serveur se lance.\nThat\u0026rsquo;s it! Une fois le serveur déployé, tu peux fermer ta connexion RDP, et télécharger le client Nice DCV.\nConnexion à l\u0026rsquo;instance via Nice DCV #Lance le client, et connecte toi via l\u0026rsquo;adresse IP publique de ton instance, sur le port 8443:\nUne erreur de certificat peut apparaître, clique sur Trust \u0026amp; Connect\nA partir de là, libre à toi d\u0026rsquo;installer tes jeux et de commencer à jouer !\nJe te conseilles de désactiver le mode de sécurité d\u0026rsquo;IE, navigateur par défaut sur Windows Server (hé oui\u0026hellip;) puis d\u0026rsquo;installer un navigateur récent. Quelques améliorations #Par défaut, le serveur NICE DCV streame 25 FPS. C\u0026rsquo;est configurable via le registre Windows, via la clée située ici : HKEY_CURRENT_USER\\Software\\GSettings\\com\\nicesoftware\\dcv. Créer une nouvelle clée display et lui donner pour valeur (type DWORD 32bits) le nombre de FPS souhaité.\nConclusion #Après quelques heures d\u0026rsquo;essais, le confort de jeu est plutôt bon. Pas trop de latences, même en WiFi. La qualité d\u0026rsquo;image est aussi plutôt bonne.\nCependant, le coût engendré par l\u0026rsquo;utilisation de l\u0026rsquo;EC2 reste trop élevé à mon sens pour que cela soit viable. En effet, il faut ajouter au prix horaire de l\u0026rsquo;instance :\nLe stockage (assez cher si on choisit de passer par du SSD : +/- 11$/mois) La bande passante : payée au Go, ça peut vite s\u0026rsquo;envoler pour du stream 4K@60FPS. Cela dit, cette méthode de tarification à l\u0026rsquo;heure peut convenir à des joueurs occasionels ne voulant pas s\u0026rsquo;abonner à un service qu\u0026rsquo;ils utiliserons que très peu.\nIl est aussi possible d\u0026rsquo;utiliser Parsec pour streamer le contenu du serveur, mais dans sa version gratuite la résolution maximale est de 1080p. Si tu remarques des fautes ou quelque chose qui ne fonctionne pas, n\u0026rsquo;hésites pas à le mentionner dans les commentaires !\nA bientôt !\n","date":"30 mars 2023","permalink":"/blog/informatique/aws-ec2-cloudgaming-instance/","section":"Blog","summary":"Provisionner facilement son instance gaming en utilisant CloudFormation et NICE DCV Server sur le cloud AWS.","title":"Provisionner sa propre instance de jeu Windows Server dans le cloud AWS via CloudFormation"},{"content":"","date":null,"permalink":"/tags/","section":"Tags","summary":"","title":"Tags"},{"content":"","date":null,"permalink":"/tags/windows-server-2019/","section":"Tags","summary":"","title":"Windows-Server-2019"},{"content":"","date":null,"permalink":"/tags/appservices/","section":"Tags","summary":"","title":"Appservices"},{"content":"","date":null,"permalink":"/tags/azure/","section":"Tags","summary":"","title":"Azure"},{"content":"","date":null,"permalink":"/categories/azure/","section":"Categories","summary":"","title":"Azure"},{"content":"","date":null,"permalink":"/tags/azuremanagedsql/","section":"Tags","summary":"","title":"Azuremanagedsql"},{"content":"Prérequis # Un compte Azure (ici j\u0026rsquo;utilise mon compte Azure Student) Terraform AzureCLI Un compte de stockage Azure, avec un containeur permettant de stocker le terraform.tfstate Dans mon cas, j\u0026rsquo;ai créé un groupe de ressource dédié à mon backend via le portail Azure, puis le compte de stockage dans ce groupe :\nCi ce n\u0026rsquo;est pas déjà fait, il faut t\u0026rsquo;authentifier à Azure via la commande az login et suivre les instructions.\nObjectif #L\u0026rsquo;objectif de cet article est d\u0026rsquo;expliquer la mise en place d\u0026rsquo;une instance GLPI dans le cloud Azure, et de sa base de donnée MySQL en utilisant des services managés proposés par Microsoft. L\u0026rsquo;intégralité du code est disponible ce repo . Cet article est basé sur le déploiement de GLPI, mais devrais fonctionner avec n\u0026rsquo;importe quelle application web fonctionnant avec une base de donnée MySQL.\nInfrastructure cible #Pour déployer notre instance de test, nous allons (dans l\u0026rsquo;ordre) définir :\nUn groupe de ressource Azure qui va contenir l\u0026rsquo;ensemble des ressources Un VPC (Virtual Network sur Azure) 2 subnets : un dédié à la base de données, l\u0026rsquo;autre à l\u0026rsquo;application web. Un serveur managé Azure mysql flexible (plan Bs1) Une base de donnée managée Azure SQL Un plan App Service Linux (B1) Une App Service Linux Une règle de pare-feu autorisant les instances Azure à accéder à la base de donnée. La base de donnée ne sera pas accessible via le web, mais uniquement via notre VPC. En fin de compte, l\u0026rsquo;architecture devrais ressembler à ceci :\nTerraform #Structure #Notre code Terraform se décompose en 3 fichiers :\n├── inputs.tf ├── main.tf └── outputs.tf inputs.tfdéfinit les variables d\u0026rsquo;entrées (adressage, nom des ressources etc). inputs.tf outputs.tfpermet d\u0026rsquo;afficher les données nécessaires pour se connecter à notre app une fois le déploiement terminé. outputs.tf main.tf contient l\u0026rsquo;ensemble du code provisionnant l\u0026rsquo;infrastructure. Je vais le détailler ci-dessous. Configuration du backend #Au début du fichier main.tf, je configure le provider Terraform azurerm, en lui indiquant où stocker le fichier terraform.statepermettant de garder en mémoire la configuration de l\u0026rsquo;infrastructure :\nterraform { backend \u0026#34;azurerm\u0026#34; { resource_group_name = \u0026#34;backend-terraform-rg\u0026#34; storage_account_name = \u0026#34;terraformbackend9809\u0026#34; container_name = \u0026#34;terraform\u0026#34; key = \u0026#34;terraform.tfstate\u0026#34; } required_providers { azurerm = { source = \u0026#34;hashicorp/azurerm\u0026#34; version = \u0026#34;~\u0026gt; 3.47.0\u0026#34; } } required_version = \u0026#34;\u0026gt;= 1.4.0\u0026#34; } provider \u0026#34;azurerm\u0026#34; { features {} } Création du groupe de ressource #Pour créer des ressources dans Azure, il faut créer un groupe de ressources :\n# Resource Group resource \u0026#34;azurerm_resource_group\u0026#34; \u0026#34;rg\u0026#34; { name = var.resource_group_name location = var.location } A noter: le code reprends les variables définies dans le fichier inputs.tf. Il en va de même pour les exemples suivants. Création du VPC et des sous-réseaux #L\u0026rsquo;exemple de code ci-dessous crée un VPC, et deux sous réseaux. Un pour la base de donnée, et un pour l\u0026rsquo;application web :\n# Virtual Network and subnet resource \u0026#34;azurerm_virtual_network\u0026#34; \u0026#34;vnet\u0026#34; { name = var.vnet_name address_space = var.vnet_address_space location = var.location resource_group_name = azurerm_resource_group.rg.name } resource \u0026#34;azurerm_subnet\u0026#34; \u0026#34;mysql_subnet\u0026#34; { name = var.mysql_subnet_name resource_group_name = azurerm_resource_group.rg.name virtual_network_name = azurerm_virtual_network.vnet.name address_prefixes = var.mysql_subnet_address_prefixes service_endpoints = [\u0026#34;Microsoft.Sql\u0026#34;] delegation { name = \u0026#34;vnet-delegation\u0026#34; service_delegation { name = \u0026#34;Microsoft.DBforMySQL/flexibleServers\u0026#34; actions = [ \u0026#34;Microsoft.Network/virtualNetworks/subnets/action\u0026#34; ] } } } resource \u0026#34;azurerm_subnet\u0026#34; \u0026#34;app_service_subnet\u0026#34; { name = var.app_subnet_name resource_group_name = azurerm_resource_group.rg.name virtual_network_name = azurerm_virtual_network.vnet.name address_prefixes = var.app_subnet_address_prefixes delegation { name = \u0026#34;vnet-delegation\u0026#34; service_delegation { name = \u0026#34;Microsoft.Web/serverFarms\u0026#34; actions = [\u0026#34;Microsoft.Network/virtualNetworks/subnets/action\u0026#34;] } } } Les délégations configurées dans les blocs delegationet service_delegationpermettent de designer ces subnets en tant que cibles pour des ressources PaaS Azure. Serveur de base de donnée SQL #J\u0026rsquo;ai choisi de déployer une base de donnée de type flexible, en utilisant le plan B_Standard_B1s. C\u0026rsquo;est suffisant pour notre infrastructure, et peu onéreux. Le tableau des tarifs est disponible ici.\nresource \u0026#34;azurerm_mysql_flexible_server\u0026#34; \u0026#34;mysql\u0026#34; { name = var.mysql_server_name location = azurerm_resource_group.rg.location resource_group_name = azurerm_resource_group.rg.name administrator_login = var.mysql_database_admin_username administrator_password = random_password.mysql_password.result backup_retention_days = 5 sku_name = \u0026#34;B_Standard_B1s\u0026#34; delegated_subnet_id = azurerm_subnet.mysql_subnet.id } Le mot de passe administrateur est généré aléatoirement et sera affiché dans les output après avoir déployé l\u0026rsquo;infrastructure. L\u0026rsquo;option\u0026rsquo; delegated_subnet_id permet d\u0026rsquo;attacher ce serveur au subnet précédement créé.\nBase de donnée SQL ## MySQL Database resource \u0026#34;azurerm_mysql_flexible_database\u0026#34; \u0026#34;mysql\u0026#34; { name = var.mysql_database_name resource_group_name = azurerm_resource_group.rg.name server_name = azurerm_mysql_flexible_server.mysql.name charset = \u0026#34;utf8\u0026#34; collation = \u0026#34;utf8_unicode_ci\u0026#34; } Cet extrait de code crée une base de donnée dédiée à GLPI, sur le serveur MySQL managé Azure.\nRègles de firewall ## Firewall rule resource \u0026#34;azurerm_mysql_flexible_server_firewall_rule\u0026#34; \u0026#34;fw-mysql\u0026#34; { name = \u0026#34;AllowAzureIPs\u0026#34; resource_group_name = azurerm_resource_group.rg.name server_name = azurerm_mysql_flexible_server.mysql.name start_ip_address = \u0026#34;0.0.0.0\u0026#34; end_ip_address = \u0026#34;0.0.0.0\u0026#34; } Par convention, autoriser les ips 0.0.0.0revient à autoriser les ressources Azure uniquement d\u0026rsquo;accéder à la base de donnée.\nPlan Azure App Service et application App Service #resource \u0026#34;azurerm_service_plan\u0026#34; \u0026#34;glpi-service-plan\u0026#34; { name = var.glpi_app_service_plan_name resource_group_name = azurerm_resource_group.rg.name location = azurerm_resource_group.rg.location os_type = \u0026#34;Linux\u0026#34; sku_name = \u0026#34;B1\u0026#34; } resource \u0026#34;azurerm_linux_web_app\u0026#34; \u0026#34;glpi-app-service\u0026#34; { name = var.glpi_app_service_name resource_group_name = azurerm_resource_group.rg.name location = azurerm_service_plan.glpi-service-plan.location service_plan_id = azurerm_service_plan.glpi-service-plan.id site_config { always_on = false application_stack { docker_image = \u0026#34;diouxx/glpi\u0026#34; docker_image_tag = \u0026#34;latest\u0026#34; } } } #Connect the Azure App to subnet resource \u0026#34;azurerm_app_service_virtual_network_swift_connection\u0026#34; \u0026#34;app\u0026#34; { app_service_id = azurerm_linux_web_app.glpi-app-service.id subnet_id = azurerm_subnet.app_service_subnet.id } Cet extrait de code déploye l\u0026rsquo;application web GLPI au sein d\u0026rsquo;un plan B1. Dans mon exemple, j\u0026rsquo;utilise une image docker GLPI hébergée sur le docker hub.\nLe plan B1 est le plan App Service le moins cher permettant d\u0026rsquo;attacher un App Service à un subnet, et ainsi pouvoir communiquer en interne avec la base de donnée. Déploiement de l\u0026rsquo;infrastructure #Une fois le code rédigé (ou téléchargé directement depuis mon repo ), le déploiement s\u0026rsquo;effectue en 3 commandes :\nterraform init terraform plan terraform apply terraform init permet de configurer le backend et récupérer le module azurerm. La commande plan permet de passer en revue les changements apportés à l\u0026rsquo;infrastructure, et apply de la déployer :\nSi tout se passe bien, la sortie de la commande apply te donneras le nécessaire pour configurer ton app GLPI :\nLe déploiement peut prendre du temps, notamment la base de donnée MySQL (+/- 10 minutes dans mon cas). L\u0026rsquo;App Service peut aussi mettre un certain temps avant d\u0026rsquo;être accessible depuis l\u0026rsquo;adresse, le temps que l\u0026rsquo;image docker soit déployée. Dernières étapes de configuration #Dans l\u0026rsquo;interface d\u0026rsquo;installation de GLPI, saisir les informations données dans l\u0026rsquo;output terraform. Si tu as utilisé mon fichier input.tf, l\u0026rsquo;utilisateur SQL sera glpi. La première tentative de connexion donnera cette erreur :\nPour la corriger, il suffit de se rendre dans les paramètres de la BDD sur le portail Azure, et de passer le paramètre require_secure_transportsur OFF:\nIl est normalement désormais possible d\u0026rsquo;installer GLPI sur la base de donnée créée pour l\u0026rsquo;occasion à l\u0026rsquo;aide de Terraform:\nVérifications #Une fois l\u0026rsquo;application installée, on se connecte à l\u0026rsquo;aide des identifiants par défaut (glpi/glpi):\nC\u0026rsquo;est fonctionnel!\nEn raison de la faible puissance de la base de donnée managée choisie, l\u0026rsquo;affichage peut être très long dans l\u0026rsquo;application. Si c\u0026rsquo;est inutilisable, ne pas hésiter à augmenter la taille de l\u0026rsquo;instance SQL. Suppression de l\u0026rsquo;infrastructure #Un simple terraform destroydétruira l\u0026rsquo;ensemble des ressources crées. A ne pas oublier, le coût mensuel de l\u0026rsquo;App Service peut être élevé.\n","date":"13 mars 2023","permalink":"/blog/informatique/azure-appservices-terraform/","section":"Blog","summary":"En prenant pour example GLPI, je t\u0026rsquo;explique comment déployer une application web complète as code en utilisant App Services","title":"Déployer GLPI 10 sur Azure App Services avec une base de donnée managée via Terraform"},{"content":"","date":null,"permalink":"/tags/glpi/","section":"Tags","summary":"","title":"Glpi"},{"content":"","date":null,"permalink":"/tags/iac/","section":"Tags","summary":"","title":"Iac"},{"content":"","date":null,"permalink":"/tags/terraform/","section":"Tags","summary":"","title":"Terraform"},{"content":"","date":null,"permalink":"/tags/latex/","section":"Tags","summary":"","title":"Latex"},{"content":"","date":null,"permalink":"/tags/markdown/","section":"Tags","summary":"","title":"Markdown"},{"content":"","date":null,"permalink":"/categories/markdown/","section":"Categories","summary":"","title":"Markdown"},{"content":"","date":null,"permalink":"/tags/pandoc/","section":"Tags","summary":"","title":"Pandoc"},{"content":"","date":null,"permalink":"/tags/pdf/","section":"Tags","summary":"","title":"Pdf"},{"content":"Le language Markdown # Attention Cet article part du principe que vous êtes déjà à l\u0026rsquo;aise avec la rédaction en Markdown. Si ce n\u0026rsquo;est pas le cas, un tutorial interactif assez bien fait est disponible ici Markdown est un language de balisage facile à utiliser et à lire. Je m\u0026rsquo;en suis beaucoup servi lors de mes études pour prendre des notes, car Markdown permet d\u0026rsquo;obtenir un résultat propre très rapidement avec un simple éditeur de texte. Différents logiciels sont compatible avec Markdown, permettant d\u0026rsquo;organiser ses notes et d\u0026rsquo;obtenir un aperçu en temps réel lors de l\u0026rsquo;édition. Personnellement, j\u0026rsquo;utilise Bear sous MacOS:\nInterface principal de Bear, avec le texte formaté en utilisant Markdown Markdown est aussi extrêmement répandu dans le monde de l\u0026rsquo;informatique, la majorité des fichiers README étant rédigé en utilisant la syntaxe Markdown.\nLes limites du Markdown #J\u0026rsquo;ai tellement pris l\u0026rsquo;habitude de rédiger mes documentations et notes en Markdown, que ça soit sur VS Code ou sur des applications type Bear, que l\u0026rsquo;utilisation de Word me paraît désormais extrêmement lourde et contre-productive. Le souci, c\u0026rsquo;est que par défaut, Markdown n\u0026rsquo;est pas prévu pour générer des documents imprimables. En effet, c\u0026rsquo;est uniquement un language de balisage, conçu pour être interprété et affiché à l\u0026rsquo;écran.\nGénérer un document type \u0026ldquo;Mémoire\u0026rdquo; depuis des fichiers Markdown #Je me suis basé sur ce repo pour créer un template (en Français) permettant de générer un document adapté à mes rendus d\u0026rsquo;école. Concrètement, il s\u0026rsquo;agit ici d\u0026rsquo;utiliser Pandoc associé à un template LaTeX permettant de générer le document. Rassurez-vous, il n\u0026rsquo;est pas nécessaire d\u0026rsquo;apprendre LaTeX pour utiliser ce projet, seule certaines commandes suffisent.\nÉtape 1 #Récupérer sur mon GitHub le template :\ngit clone https://github.com/fabienchevalier/phd_thesis_markdown.git Pandoc nécessite LaTeX pour générer des PDF. En fonction de votre OS, il faut donc installer une distribution LaTeX:\nLinux: sudo apt-get install textlive Windows: voir du côté de MiKTex MacOS: via homebrew avec brew install --cask mactex Par la suite, mettre à jour sa distribution LaTeX:\nsudo tlmgr update --self Étape 2 #Installer Pandoc dans un environnement Python séparé. Personnellement, j\u0026rsquo;utilise miniconda sous MacOS (disponible aussi sur Linux/Windows):\nbrew install miniconda #apt-get install miniconda devrais fonctionner sous Ubuntu/Debian conda create -n phd -y python=3.7 pandoc conda activate phd Les autres dépendances peuvent être installées automatiquement via le Makefile proposé :\nmake install Étape 3 #Par la suite, déposer les fichiers md dans le dossier source et executer make pdf pour générer le document au format PDF. Ne pas oublier de modifier le fichier metadata.yml dans le dossier source en fonction des besoins! Un exemple de rendu est disponible ici.\n","date":"7 février 2023","permalink":"/blog/informatique/markdown-to-pdf-latex/","section":"Blog","summary":"Comment générer un document type \u0026ldquo;Mémoire\u0026rdquo; au format PDF depuis des fichiers Markdown en utilisant LaTeX et Pandoc","title":"Rédiger son mémoire universitaire en Markdown"},{"content":" Réalisé dans le cadre de mes deux années de BTS, ce portofolio résume les travaux entrepris lors de ces deux années d\u0026rsquo;étude. Entièrement réalisé en Markdown, ce site web est hébergé par GitHub et à été construit à partir du template Jekyll Minimal Mistake.\nhttps://fchevalier.net/bts\n","date":"1 septembre 2022","permalink":"/projets/bts_sio/","section":"Projets","summary":"Réalisé dans le cadre de mes deux années de BTS, ce portofolio résume les travaux entrepris lors de ces deux années d\u0026rsquo;étude.","title":"Portofolio BTS SIO SISR"}]